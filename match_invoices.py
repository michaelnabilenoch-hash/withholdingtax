import io
import re
from itertools import combinations
import numpy as np
import pandas as pd
import streamlit as st
from difflib import SequenceMatcher

# ============================================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
# ============================================================
COL_INV = "ÙÙˆØ§ØªÙŠØ±"
COL_DATE = "Ø§Ù„ØªØ§Ø±ÙŠØ®"
COL_NAME = "Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©"
COL_AMOUNT = "ØµØ§ÙÙ‰ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª"
COL_REG = "Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"  # Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ

COL_TAX_NAME = "Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©"
COL_TAX_AMOUNT = "Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØµØ§ÙÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„"
COL_TAX_TAXED = "Ù…Ø­ØµÙ„ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¶Ø±ÙŠØ¨Ù‡"
COL_TAX_RATE = "Ù†Ø³Ø¨Ø© Ø§Ù„Ø®ØµÙ…"
COL_TAX_DATE = "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø§Ù…Ù„"
COL_TAX_REG = "Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"  # Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø®ØµÙ…

NEW_COLS = [
    "Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø±Ù‚Ù… Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ø³Ù†Ø© Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ù…Ø¨Ù„Øº Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ù„ØªØ­Ù‚Ù‚",
    "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ù† Ø§Ù„Ù…Ø±ØªØ¬Ø¹",
]

# ============================================================
# WORD_MAP & STOPWORDS
# ============================================================
WORD_MAP = {
    "Ø§Ù„Ù…ÙŠØ§Ù‡": "Ù…ÙŠØ§Ù‡", "Ø§Ù„Ù…ÙŠØ§Ø©": "Ù…ÙŠØ§Ù‡", "Ù…ÙŠØ§Ø©": "Ù…ÙŠØ§Ù‡", "Ø§Ù„Ù…Ø§Ø¡": "Ù…ÙŠØ§Ù‡",
    "Ø§Ù„ØµØ±Ù": "ØµØ±Ù", "Ø§Ù„ØµØ±Ù Ø§Ù„ØµØ­ÙŠ": "ØµØ±Ù ØµØ­ÙŠ", "ØµØ±Ù Ø§Ù„ØµØ­ÙŠ": "ØµØ±Ù ØµØ­ÙŠ",
    "Ø§Ù„Ø´Ø±Ø¨": "Ø´Ø±Ø¨", "Ø§Ù„Ø´Ø±Ø§Ø¨": "Ø´Ø±Ø¨",
    "Ø¨Ø³ÙˆÙ‡Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬", "Ø¨Ø³ÙˆÙ‡Ù€Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬", "Ø³ÙˆÙ‡Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬",
    "Ø§Ù„Ø²Ø±Ø§Ø¹Ù‰": "Ø²Ø±Ø§Ø¹ÙŠ", "Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ": "Ø²Ø±Ø§Ø¹ÙŠ", "Ø²Ø±Ø§Ø¹ÙŠØ©": "Ø²Ø±Ø§Ø¹ÙŠ",
    "Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±": "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ©": "Ø§Ø³ØªØ«Ù…Ø§Ø±",
    "Ù„Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª", "Ø§Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª",
    "Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª": "ØµÙ†Ø§Ø¹Ø§Øª", "Ø§Ù„ØµÙ†Ø§Ø¹Ø§Øª": "ØµÙ†Ø§Ø¹Ø§Øª",
    "Ù„Ù„ØªÙˆØ±ÙŠØ¯Ø§Øª": "ØªÙˆØ±ÙŠØ¯Ø§Øª", "ØªÙˆØ±ÙŠØ¯": "ØªÙˆØ±ÙŠØ¯Ø§Øª",
    "Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©": "ØºØ°Ø§Ø¦ÙŠØ©", "Ø§ØºØ°ÙŠØ©": "ØºØ°Ø§Ø¦ÙŠØ©", "Ø§ØºØ°ÙŠÙ‡": "ØºØ°Ø§Ø¦ÙŠØ©",
}

STOPWORDS = {
    "Ø´Ø±ÙƒØ©", "Ø§Ù„Ø´Ø±ÙƒØ©", "Ø´Ø±ÙƒÙ‡", "Ø§Ù„Ø´Ø±ÙƒÙ‡",
    "ÙˆØ§Ù„", "Ø¨Ø§Ù„", "Ù„Ù„", "Ù„", "Ùˆ",
    "Ù…ØµØ±", "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©", "Ù…ØµØ±ÙŠØ©",
    "Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©", "Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©", "Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©",
    "Ù…ØµÙ†Ø¹", "ØµÙ†Ø§Ø¹ÙŠØ©", "ØªØ¬Ø§Ø±ÙŠØ©",
    "Ø¬Ø±ÙˆØ¨", "Ù…Ø¬Ù…ÙˆØ¹Ø©",
}

def normalize_letters(text):
    if pd.isna(text): return ""
    s = str(text)
    s = re.sub(r"[Ø£Ø¥Ø¢Ø§]", "Ø§", s)
    s = re.sub(r"[Ø©]", "Ù‡", s)
    s = re.sub(r"[Ù‰ÙŠØ¦]", "ÙŠ", s)
    s = re.sub(r"[Ø¤]", "Ùˆ", s)
    s = re.sub(r"[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù€]", "", s)
    return s

def remove_al_prefix(word):
    for pref in ("ÙˆØ§Ù„", "Ø¨Ø§Ù„", "Ù„Ù„", "Ø§Ù„", "Ù„"):
        if word.startswith(pref) and len(word) > len(pref) + 1:
            return word[len(pref):]
    return word

def normalize_name(s):
    if pd.isna(s): return ""
    s = normalize_letters(s).lower()
    s = re.sub(r"[^Ø¡-ÙŠ\s]", " ", s)
    words = [remove_al_prefix(w) for w in s.split() if w.strip()]
    normalized = [WORD_MAP.get(w, w) for w in words]
    final = " ".join(normalized)
    for k, v in WORD_MAP.items():
        final = re.sub(rf"\b{k}\b", v, final)
    return re.sub(r"\s+", " ", final).strip()

def tokenize(s):
    norm = normalize_name(s)
    return set(w for w in norm.split() if w and w not in STOPWORDS)

def normalize_reg_number(reg):
    """ØªÙ†Ø¸ÙŠÙ Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ"""
    if pd.isna(reg): return ""
    s = str(reg).strip()
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø´Ø±Ø·Ø§Øª
    s = re.sub(r"[\s\-_]", "", s)
    return s

def fuzzy(a, b):
    return SequenceMatcher(None, a, b).ratio()

def to_num(v):
    if pd.isna(v): return np.nan
    s = str(v).strip()
    if not s or s.startswith("#"): return np.nan
    try:
        return float(s.replace(",", ""))
    except:
        return np.nan

def parse_dates(series, dayfirst):
    dt = pd.to_datetime(series, errors="coerce", dayfirst=dayfirst)
    return dt, dt.dt.year.fillna(0).astype(int), dt.dt.month.fillna(0).astype(int)

# ============================================================
# ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù„ÙØ§Øª - Ù…Ø¹ Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø­Ø±Ø¬
# ============================================================
def prepare_sales(df_raw):
    df = df_raw.copy()
    df["amt"] = df[COL_AMOUNT].apply(to_num)
    
    # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    if COL_REG in df.columns:
        df["reg_clean"] = df[COL_REG].apply(normalize_reg_number)
    else:
        df["reg_clean"] = ""
    
    grouped = df.groupby(COL_INV).agg(
        net_amount=("amt", "sum"),
        pos_date=(
            COL_DATE,
            lambda x: x[df.loc[x.index, "amt"] > 0].iloc[0]
            if any(df.loc[x.index, "amt"] > 0) else np.nan
        ),
        has_return=("amt", lambda s: any(s < 0)),
        name=(COL_NAME, "first"),
        reg_clean=("reg_clean", "first"),  # Ø¥Ø¶Ø§ÙØ© Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    ).reset_index()
    
    grouped = grouped[grouped["net_amount"] > 0]
    
    # ğŸ”¥ Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø­Ø±Ø¬: Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¨ØµÙŠØºØ© dd/mm/yyyy
    grouped["date_parsed"], grouped["year"], grouped["month"] = parse_dates(
        grouped["pos_date"], dayfirst=True  # âœ… ØªÙ… ØªØµØ­ÙŠØ­Ù‡!
    )
    
    grouped["name_norm"] = grouped["name"].apply(normalize_name)
    grouped["tokens"] = grouped["name"].apply(tokenize)
    return grouped

def prepare_tax(df_raw):
    df = df_raw.copy()
    df["v_file"] = df[COL_TAX_AMOUNT].apply(to_num)
    df["v_tax_paid"] = df[COL_TAX_TAXED].apply(to_num)
    
    # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    if COL_TAX_REG in df.columns:
        df["reg_clean"] = df[COL_TAX_REG].apply(normalize_reg_number)
    else:
        df["reg_clean"] = ""
    
    def rate_to_float(x):
        try:
            return float(str(x).replace("%", "").strip()) / 100.0
        except:
            return np.nan
    
    df["rate"] = df[COL_TAX_RATE].apply(rate_to_float)
    df["v_tax"] = df.apply(
        lambda r: r["v_tax_paid"] / r["rate"]
        if pd.notna(r["v_tax_paid"]) and pd.notna(r["rate"]) and r["rate"] > 0
        else np.nan,
        axis=1
    )
    df["v_mix"] = df[["v_file", "v_tax"]].mean(axis=1, skipna=True)
    df["date_parsed"], df["year"], df["month"] = parse_dates(
        df[COL_TAX_DATE], dayfirst=True
    )
    df["name_norm"] = df[COL_TAX_NAME].apply(normalize_name)
    df["tokens"] = df[COL_TAX_NAME].apply(tokenize)
    return df

# ============================================================
# ÙÙ„Ø§ØªØ± Ø§Ù„Ø¨Ø­Ø«
# ============================================================
def filter_year_and_date(sales_df, tax_date, tax_year, tax_month):
    if tax_year == 0 or pd.isna(tax_date):
        return sales_df.iloc[0:0]
    
    # Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ ÙÙŠ 3 Ø³Ù†ÙˆØ§Øª
    allowed_years = [tax_year, tax_year - 1, tax_year - 2]
    mask_year = sales_df["year"].isin(allowed_years)
    mask_date = (sales_df["date_parsed"] <= tax_date)
    
    return sales_df[mask_year & mask_date]

def extended_subset_search(cand, targets, max_invoices=25, max_nodes=200000):
    """
    ÙŠØ­Ø§ÙˆÙ„ Ø¥ÙŠØ¬Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ÙÙˆØ§ØªÙŠØ± (Ø£ÙŠ Ø¹Ø¯Ø¯) Ù…Ø¬Ù…ÙˆØ¹Ù‡Ø§ Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø£Ø­Ø¯ Ø§Ù„Ù€ targets
    Ù…Ø¹ Ø­Ø¯ÙˆØ¯ max_invoices (Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ ÙÙˆØ§ØªÙŠØ± Ù†Ø¬Ø±Ø¨Ù‡) Ùˆ max_nodes (Ø£Ù‚ØµÙ‰ Ø¹ÙÙ‚Ø¯ Ø¨Ø­Ø«)
    """
    if not targets:
        return None

    max_t, min_t = max(targets), min(targets)

    # Ù†Ù‚ØªØµØ± Ø¹Ù„Ù‰ Ø£ÙˆÙ„ max_invoices ÙØ§ØªÙˆØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ±ØªÙŠØ¨ ØªÙ†Ø§Ø²Ù„ÙŠ Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø©
    cand = cand.sort_values("net_amount", ascending=False).head(max_invoices)

    rows = list(cand.itertuples(index=False))
    n = len(rows)
    if n == 0:
        return None

    amounts = [r.net_amount for r in rows]

    # suffix sums Ø¹Ù„Ø´Ø§Ù† Ù†Ø¹Ø±Ù Ø£Ù‚ØµÙ‰ Ù…Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØªÙ‡ ÙÙŠ Ø§Ù„ÙØ±ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© (Ù„Ù„Ù‚Øµ pruning)
    suffix = [0.0] * (n + 1)
    for i in range(n - 1, -1, -1):
        suffix[i] = suffix[i + 1] + amounts[i]

    best = None
    best_diff = float("inf")
    nodes = 0

    def dfs(i, cur_sum, chosen):
        nonlocal best, best_diff, nodes
        nodes += 1

        # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ø¹ÙÙ‚Ø¯ Ø§Ù„Ø¨Ø­Ø«
        if nodes > max_nodes:
            return

        # Ù„Ùˆ ØªØ¬Ø§ÙˆØ²Ù†Ø§ Ø£Ø¹Ù„Ù‰ target Ø¨Ù‡Ø§Ù…Ø´ 5% Ù†ÙˆÙ‚Ù Ù‡Ø°Ø§ Ø§Ù„ÙØ±Ø¹
        if cur_sum > max_t * 1.05:
            return

        # Ù„Ùˆ Ø­ØªÙ‰ Ù„Ùˆ Ø£Ø®Ø°Ù†Ø§ ÙƒÙ„ Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ù…Ø´ Ù‡Ù†ÙˆØµÙ„ Ù„Ù€ 95% Ù…Ù† Ø£Ù‚Ù„ target â†’ Ø§Ù„ÙØ±Ø¹ Ø¯Ù‡ Ù…Ù„ÙˆØ´ Ù„Ø§Ø²Ù…Ø©
        if cur_sum + suffix[i] < min_t * 0.95:
            return

        if i == n:
            diff = min(abs(cur_sum - t) for t in targets)
            if diff <= 0.05 * max_t and diff < best_diff:
                best_diff = diff
                best = chosen[:]
            return

        # 1) Ù†Ø¬Ø±Ø¨ Ù†Ø£Ø®Ø° Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        chosen.append(i)
        dfs(i + 1, cur_sum + amounts[i], chosen)
        chosen.pop()

        # 2) Ù†Ø¬Ø±Ø¨ Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        dfs(i + 1, cur_sum, chosen)

    dfs(0, 0.0, [])
    return [rows[i] for i in best] if best else None

# ============================================================
# Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ============================================================
def find_best_match(tax_row, sales_df, used_invoices):
    tax_date = tax_row["date_parsed"]
    if pd.isna(tax_date):
        return None

    v_file, v_tax, v_mix = tax_row["v_file"], tax_row["v_tax"], tax_row["v_mix"]
    targets = [t for t in (v_file, v_tax, v_mix) if pd.notna(t) and t > 0]
    if not targets:
        return None

    # ÙÙˆØ§ØªÙŠØ± Ù†ÙØ³ Ø§Ù„ÙØªØ±Ø©
    cand = filter_year_and_date(sales_df, tax_date, tax_row["year"], tax_row["month"])
    if cand.empty:
        return None

    # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù„ÙŠ Ø§ØªØ³ØªØ®Ø¯Ù…Øª Ù‚Ø¨Ù„ ÙƒØ¯Ù‡
    cand = cand[~cand[COL_INV].astype(str).isin(used_invoices)].copy()
    if cand.empty:
        return None

    # ØªØµÙÙŠØ© Ø¨Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    tax_reg = str(tax_row.get("reg_clean", "")).strip()
    if tax_reg:
        cand_with_reg = cand[cand["reg_clean"] == tax_reg]
        if not cand_with_reg.empty:
            cand = cand_with_reg.copy()

    if cand.empty:
        return None

    # Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø§Ø³Ù…
    cand["token_score"] = cand["tokens"].apply(lambda t: len(t & tax_row["tokens"]))
    cand["fuzzy"] = cand["name_norm"].apply(lambda s: fuzzy(s, tax_row["name_norm"]))
    cand = cand[(cand["token_score"] >= 1) | (cand["fuzzy"] >= 0.70)]
    if cand.empty:
        return None

    def within_absolute(val, max_diff=5.0):
        return any(abs(val - t) <= max_diff for t in targets)

    def within_pct(val, pct=0.05):
        return any(abs(val - t) <= pct * t for t in targets)

    cand["value_dist"] = cand["net_amount"].apply(
        lambda x: min(abs(x - t) for t in targets)
    )
    cand["reg_match"] = (cand["reg_clean"] == tax_reg) & (tax_reg != "")

    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
    cand = cand.sort_values(
        by=["reg_match", "value_dist", "token_score", "fuzzy"],
        ascending=[False, True, False, False]
    )

    # ğŸ†• (0) Ù„Ùˆ ÙÙŠÙ‡ Ø±Ù‚Ù… ØªØ³Ø¬ÙŠÙ„: Ø¬Ø±Ù‘Ø¨ Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ù„Ù†ÙØ³ Ø§Ù„Ø±Ù‚Ù… Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
    if tax_reg and not cand.empty:
        total_reg = cand["net_amount"].sum()
        if within_absolute(total_reg, 5.0) or within_pct(total_reg):
            invs = cand[COL_INV].astype(str).tolist()
            years = cand["year"].astype(str).tolist()
            dates = cand["pos_date"].astype(str).tolist()
            has_ret = cand["has_return"].any()
            return invs, years, dates, float(total_reg), has_ret

    # 1ï¸âƒ£ ÙØ§ØªÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©
    for _, r in cand.head(100).iterrows():
        if within_absolute(r["net_amount"], max_diff=5.0) or within_pct(r["net_amount"]):
            return (
                [str(r[COL_INV])],
                [str(r["year"])],
                [str(r["pos_date"])],
                float(r["net_amount"]),
                r["has_return"],
            )

    # 2ï¸âƒ£ Ù…Ø¬Ù…ÙˆØ¹ 2 ÙÙˆØ§ØªÙŠØ±
    for combo in combinations(cand.head(60).itertuples(index=False), 2):
        total = sum(r.net_amount for r in combo)
        if not (within_absolute(total, 5.0) or within_pct(total)):
            continue
        invs = [str(r._asdict()[COL_INV]) for r in combo]
        if len(set(invs)) != len(invs):
            continue
        years = [str(r.year) for r in combo]
        dates = [str(r.pos_date) for r in combo]
        ret = any(r.has_return for r in combo)
        return invs, years, dates, float(total), ret

    # 3ï¸âƒ£ Ù…Ø¬Ù…ÙˆØ¹ 3 ÙÙˆØ§ØªÙŠØ±
    for combo in combinations(cand.head(60).itertuples(index=False), 3):
        total = sum(r.net_amount for r in combo)
        if not within_pct(total):
            continue
        invs = [str(r._asdict()[COL_INV]) for r in combo]
        if len(set(invs)) != len(invs):
            continue
        years = [str(r.year) for r in combo]
        dates = [str(r.pos_date) for r in combo]
        ret = any(r.has_return for r in combo)
        return invs, years, dates, float(total), ret

    # 4ï¸âƒ£ Ø¨Ø­Ø« Ø¹Ø§Ù… Ù„Ø£ÙŠ Ø¹Ø¯Ø¯ ÙÙˆØ§ØªÙŠØ± (4 ÙØ£ÙƒØ«Ø±) Ù…Ø¹ Ø­Ø¯ÙˆØ¯ Ù…Ù†Ø·Ù‚ÙŠØ©
    max_invoices = 25  # ØªÙ‚Ø¯Ø± ØªØ²ÙˆØ¯Ù‡Ø§ Ù„Ù€ 30 Ù„Ùˆ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ù…Ø´ Ø¶Ø®Ù…Ø©
    ext = extended_subset_search(
        cand,
        targets,
        max_invoices=max_invoices,
        max_nodes=200000
    )
    if ext:
        total = sum(r.net_amount for r in ext)
        if within_pct(total):
            invs = [str(r._asdict()[COL_INV]) for r in ext]
            years = [str(r.year) for r in ext]
            dates = [str(r.pos_date) for r in ext]
            ret = any(r.has_return for r in ext)
            return invs, years, dates, float(total), ret

    return None

def match_all_basic(sales_df, tax_df):
    used = set()
    result = tax_df.copy()
    for col in NEW_COLS:
        result[col] = ""
    
    matched = 0
    for idx, row in result.iterrows():
        res = find_best_match(row, sales_df, used)
        if res:
            invs, years, dates, amt, has_ret = res
            result.at[idx, NEW_COLS[0]] = " + ".join(invs)
            result.at[idx, NEW_COLS[1]] = " + ".join(years)
            result.at[idx, NEW_COLS[2]] = " + ".join(dates)
            result.at[idx, NEW_COLS[3]] = amt
            result.at[idx, NEW_COLS[4]] = "Ù„Ù‡ Ù…Ø±ØªØ¬Ø¹" if has_ret else ""
            used.update(invs)
            matched += 1
    
    return result, matched, len(result) - matched

# ============================================================
# Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª
# ============================================================
def match_with_user_feedback(sales_df_original, tax_df_original, matches_edited, stopwords_edited):
    global STOPWORDS
    
    # ØªØ­Ø¯ÙŠØ« STOPWORDS
    if "ÙƒÙ„Ù…Ø©" in stopwords_edited.columns:
        words = [str(v).strip() for v in stopwords_edited["ÙƒÙ„Ù…Ø©"].tolist() if str(v).strip()]
        STOPWORDS = set(words)
    
    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¬Ù‡ÙŠØ² tokens
    sales_df = sales_df_original.copy()
    tax_df = tax_df_original.copy()
    
    sales_df["name_norm"] = sales_df["name"].apply(normalize_name)
    sales_df["tokens"] = sales_df["name"].apply(tokenize)
    
    tax_df["name_norm"] = tax_df[COL_TAX_NAME].apply(normalize_name)
    tax_df["tokens"] = tax_df[COL_TAX_NAME].apply(tokenize)
    
    result = tax_df.copy()
    for col in NEW_COLS:
        result[col] = ""
    
    used = set()
    
    # ØªØ«Ø¨ÙŠØª Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©
    if matches_edited is not None and not matches_edited.empty and "row_id" in matches_edited.columns:
        for _, r in matches_edited.iterrows():
            if "Ø§Ø¹ØªÙ…Ø§Ø¯_Ø§Ù„ØªØ·Ø§Ø¨Ù‚" in matches_edited.columns:
                if not bool(r.get("Ø§Ø¹ØªÙ…Ø§Ø¯_Ø§Ù„ØªØ·Ø§Ø¨Ù‚", True)):
                    continue
            
            row_id = int(r["row_id"])
            inv_str = str(r[NEW_COLS[0]]).strip()
            if not inv_str:
                continue
            
            invs = [x.strip() for x in inv_str.split("+") if x.strip()]
            years = str(r.get(NEW_COLS[1], "")).split("+")
            dates = str(r.get(NEW_COLS[2], "")).split("+")
            amt = r.get(NEW_COLS[3], np.nan)
            note = r.get(NEW_COLS[4], "")
            
            result.at[row_id, NEW_COLS[0]] = " + ".join(invs)
            result.at[row_id, NEW_COLS[1]] = " + ".join([y.strip() for y in years])
            result.at[row_id, NEW_COLS[2]] = " + ".join([d.strip() for d in dates])
            result.at[row_id, NEW_COLS[3]] = amt
            result.at[row_id, NEW_COLS[4]] = note
            
            used.update(invs)
    
    # Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¨Ø§Ù‚ÙŠ
    matched = 0
    for idx, row in result.iterrows():
        if str(result.at[idx, NEW_COLS[0]]).strip():
            matched += 1
            continue
        
        res = find_best_match(row, sales_df, used)
        if res:
            invs, years, dates, amt, has_ret = res
            result.at[idx, NEW_COLS[0]] = " + ".join(invs)
            result.at[idx, NEW_COLS[1]] = " + ".join(years)
            result.at[idx, NEW_COLS[2]] = " + ".join(dates)
            result.at[idx, NEW_COLS[3]] = amt
            result.at[idx, NEW_COLS[4]] = "Ù„Ù‡ Ù…Ø±ØªØ¬Ø¹" if has_ret else ""
            used.update(invs)
            matched += 1
    
    return result, matched, len(result) - matched

# ============================================================
# ÙˆØ§Ø¬Ù‡Ø© Streamlit
# ============================================================
st.set_page_config(page_title="Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹", layout="wide")

st.title("ğŸ¯ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ - Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø·ÙˆØªÙŠÙ† Ø§Ù„Ù…Ø­Ø³Ù‘Ù†")
st.markdown("---")

with st.expander("ğŸ“– ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…", expanded=True):
    st.markdown("""
    ### Ø§Ù„Ø®Ø·ÙˆØ© 1ï¸âƒ£: Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©
    - Ø­Ù…Ù‘Ù„ Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª ÙˆÙƒØ´Ù Ø§Ù„Ø®ØµÙ…
    - Ø§Ø¶ØºØ· "Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø¯Ø¦ÙŠØ©"
    - Ø³ØªØ¸Ù‡Ø± Ù„Ùƒ Ø¬Ø¯Ø§ÙˆÙ„ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©:
      * Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª (ÙŠÙ…ÙƒÙ†Ùƒ Ø­Ø°Ù Ø§Ù„Ø®Ø§Ø·Ø¦Ø© Ø£Ùˆ Ø¥Ù„ØºØ§Ø¡ Ø§Ø¹ØªÙ…Ø§Ø¯Ù‡Ø§)
      * Ø¬Ø¯ÙˆÙ„ STOPWORDS (Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©)
    
    ### Ø§Ù„Ø®Ø·ÙˆØ© 2ï¸âƒ£: Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    - Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ¹Ø¯Ù‘Ù„ ÙÙŠÙ‡Ø§
    - Ø§Ø¶ØºØ· "Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"
    - Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ«Ø¨Øª Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø© ÙˆÙŠÙƒÙ…Ù„ Ø§Ù„Ø¨Ø§Ù‚ÙŠ
    
    ### ğŸ†• Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ
    - Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª ÙˆÙƒØ´Ù Ø§Ù„Ø®ØµÙ… ÙŠØ­ØªÙˆÙŠØ§Ù† Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ **"Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"**
    - Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø³ÙŠØ¹Ø·ÙŠ **Ø£ÙˆÙ„ÙˆÙŠØ© Ù‚ØµÙˆÙ‰** Ù„Ù„ÙÙˆØ§ØªÙŠØ± Ø¨Ù†ÙØ³ Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    - ÙƒÙ…Ø§ Ø³ÙŠØ¬Ø±Ø¨ Ø£ÙˆÙ„Ø§Ù‹ ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ ÙÙˆØ§ØªÙŠØ± Ù†ÙØ³ Ø§Ù„Ø±Ù‚Ù… ÙÙŠ Ø§Ù„ÙØªØ±Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù‚ÙŠÙ…Ø© ÙƒØ´Ù Ø§Ù„Ø®ØµÙ…
    """)

col1, col2 = st.columns(2)
with col1:
    sales_file = st.file_uploader("ğŸ“Š Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª (CSV)", type="csv")
with col2:
    tax_file = st.file_uploader("ğŸ“‘ ÙƒØ´Ù Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ (CSV)", type="csv")

st.markdown("---")

# Ø§Ù„Ø®Ø·ÙˆØ© 1
if st.button("ğŸš€ Ø§Ù„Ø®Ø·ÙˆØ© 1: Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø¯Ø¦ÙŠØ©", use_container_width=True, type="primary"):
    if not sales_file or not tax_file:
        st.error("âš ï¸ Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙÙŠÙ† Ø£ÙˆÙ„Ø§Ù‹!")
        st.stop()
    
    try:
        with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©..."):
            sales_raw = pd.read_csv(sales_file, encoding="utf-8-sig", dtype=str)
            tax_raw = pd.read_csv(tax_file, encoding="utf-8-sig", dtype=str)
            
            sales_prepared = prepare_sales(sales_raw)
            tax_prepared = prepare_tax(tax_raw)
            
            draft_df, ok, bad = match_all_basic(sales_prepared, tax_prepared)
        
        st.session_state["sales_prepared"] = sales_prepared
        st.session_state["tax_prepared"] = tax_prepared
        
        draft_df = draft_df.copy()
        draft_df.insert(0, "row_id", draft_df.index.astype(int))
        st.session_state["draft_df"] = draft_df
        
        matches_only = draft_df[draft_df[NEW_COLS[0]] != ""].copy()
        matches_only["Ø§Ø¹ØªÙ…Ø§Ø¯_Ø§Ù„ØªØ·Ø§Ø¨Ù‚"] = True
        st.session_state["matches_table"] = matches_only
        
        stopwords_df = pd.DataFrame({"ÙƒÙ„Ù…Ø©": sorted(STOPWORDS)})
        st.session_state["stopwords_table"] = stopwords_df
        
        success_rate = (ok/(ok+bad)*100) if (ok+bad) > 0 else 0
        st.success(f"âœ… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©: {ok:,} Ù…Ø·Ø§Ø¨Ù‚ ({success_rate:.1f}%) | {bad:,} ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚")
        st.info("â¬‡ Ø§Ù†Ø²Ù„ Ù„Ù„Ø£Ø³ÙÙ„ Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„")
        
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£: {str(e)}")
        st.exception(e)

st.markdown("---")

# Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
if "draft_df" in st.session_state:
    st.subheader("ğŸ§¾ Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©")
    matches_df = st.session_state.get("matches_table", pd.DataFrame())
    
    if not matches_df.empty:
        edited_matches = st.data_editor(
            matches_df,
            key="matches_editor",
            num_rows="dynamic",
            use_container_width=True,
            column_config={
                "Ø§Ø¹ØªÙ…Ø§Ø¯_Ø§Ù„ØªØ·Ø§Ø¨Ù‚": st.column_config.CheckboxColumn(
                    "Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„ØªØ·Ø§Ø¨Ù‚",
                    help="Ø£Ù„ØºÙ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù„Ø±ÙØ¶ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø§Ø¨Ù‚",
                    default=True,
                )
            }
        )
    
    st.subheader("ğŸ§¹ ÙƒÙ„Ù…Ø§Øª STOPWORDS Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©")
    stopwords_df = st.session_state.get("stopwords_table", pd.DataFrame())
    edited_stopwords = st.data_editor(
        stopwords_df,
        key="stopwords_editor",
        num_rows="dynamic",
        use_container_width=True
    )
    
    st.markdown("---")
    
    # Ø§Ù„Ø®Ø·ÙˆØ© 2
    if st.button("âœ… Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©", use_container_width=True, type="primary"):
        try:
            sales_prepared = st.session_state["sales_prepared"]
            tax_prepared = st.session_state["tax_prepared"]
            
            with st.spinner("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©..."):
                final_df, ok2, bad2 = match_with_user_feedback(
                    sales_prepared,
                    tax_prepared,
                    edited_matches if 'edited_matches' in locals() else matches_df,
                    edited_stopwords if 'edited_stopwords' in locals() else stopwords_df,
                )
            
            success_rate = (ok2/(ok2+bad2)*100) if (ok2+bad2) > 0 else 0
            
            st.success("ğŸ‰ ØªÙ…Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©!")
            
            c1, c2, c3 = st.columns(3)
            with c1:
                st.metric("âœ… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚", f"{ok2:,}", delta=f"{success_rate:.1f}%")
            with c2:
                st.metric("âŒ ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚", f"{bad2:,}")
            with c3:
                st.metric("ğŸ“ˆ Ø§Ù„Ù†Ø¬Ø§Ø­", f"{success_rate:.2f}%")
            
            st.markdown("---")
            
            col1, col2 = st.columns(2)
            with col1:
                out = io.BytesIO()
                final_df.to_csv(out, index=False, encoding="utf-8-sig")
                st.download_button(
                    "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ´Ù Ø§Ù„ÙƒØ§Ù…Ù„",
                    data=out.getvalue(),
                    file_name="ÙƒØ´Ù_Ù…Ø·Ø§Ø¨Ù‚_Ù†Ù‡Ø§Ø¦ÙŠ.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            
            with col2:
                unmatched = final_df[final_df[NEW_COLS[0]] == ""]
                if not unmatched.empty:
                    out2 = io.BytesIO()
                    unmatched.to_csv(out2, index=False, encoding="utf-8-sig")
                    st.download_button(
                        "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚",
                        data=out2.getvalue(),
                        file_name="ØºÙŠØ±_Ù…Ø·Ø§Ø¨Ù‚.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
            
            st.markdown("### ğŸ‘€ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
            st.dataframe(final_df.head(20), use_container_width=True)
            
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£: {str(e)}")
            st.exception(e)

st.markdown("---")
st.caption("ğŸ’¼ Ù…Ø­Ø§Ø³Ø¨ Ù‚Ø§Ù†ÙˆÙ†ÙŠ: Ù…Ø§ÙŠÙƒÙ„ Ù†Ø¨ÙŠÙ„ | ğŸš€ 2025")
