import io
import re
from itertools import combinations
import numpy as np
import pandas as pd
import streamlit as st
from difflib import SequenceMatcher

# ============================================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
# ============================================================
COL_INV = "ÙÙˆØ§ØªÙŠØ±"
COL_DATE = "Ø§Ù„ØªØ§Ø±ÙŠØ®"
COL_NAME = "Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©"
COL_AMOUNT = "ØµØ§ÙÙ‰ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª"

COL_TAX_NAME = "Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©"
COL_TAX_AMOUNT = "Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØµØ§ÙÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„"
COL_TAX_TAXED = "Ù…Ø­ØµÙ„ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¶Ø±ÙŠØ¨Ù‡"
COL_TAX_RATE = "Ù†Ø³Ø¨Ø© Ø§Ù„Ø®ØµÙ…"
COL_TAX_DATE = "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø§Ù…Ù„"

NEW_COLS = [
    "Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø±Ù‚Ù… Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ø³Ù†Ø© Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ù…Ø¨Ù„Øº Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ù„ØªØ­Ù‚Ù‚",
    "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ù† Ø§Ù„Ù…Ø±ØªØ¬Ø¹",
]

# ============================================================
# Ø¯ÙˆØ§Ù„ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ / WORD_MAP + STOPWORDS
# ============================================================
WORD_MAP = {

    # ====================
    #  Ø­Ø±ÙˆÙ ÙˆØªØ¨Ø¯ÙŠÙ„Ø§Øª Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©
    # ====================
    "Ø§Ù„Ù…ÙŠØ§Ù‡": "Ù…ÙŠØ§Ù‡", "Ø§Ù„Ù…ÙŠØ§Ø©": "Ù…ÙŠØ§Ù‡", "Ù…ÙŠØ§Ø©": "Ù…ÙŠØ§Ù‡",
    "Ø§Ù„Ù…Ø§Ø¡": "Ù…ÙŠØ§Ù‡", "Ù…Ø§Ø¦": "Ù…ÙŠØ§Ù‡",

    "Ø§Ù„ØµØ±Ù": "ØµØ±Ù", "Ø§Ù„ØµØ±Ù‚": "ØµØ±Ù",
    "Ø§Ù„ØµØ±Ù Ø§Ù„ØµØ­ÙŠ": "ØµØ±Ù ØµØ­ÙŠ", "Ø§Ù„ØµØ±Ù Ø§Ù„ØµØ­Ù‰": "ØµØ±Ù ØµØ­ÙŠ",
    "ØµØ±Ù Ø§Ù„ØµØ­ÙŠ": "ØµØ±Ù ØµØ­ÙŠ", "ØµØ±Ù Ø§Ù„ØµØ­Ù‰": "ØµØ±Ù ØµØ­ÙŠ",

    "Ø§Ù„Ø´Ø±Ø¨": "Ø´Ø±Ø¨", "Ø§Ù„Ø´Ø±Ø§Ø¨": "Ø´Ø±Ø¨", "Ø´Ø±Ø§Ø¨": "Ø´Ø±Ø¨",

    "Ø§Ù„ÙˆØ­Ø¯Ø§Øª": "ÙˆØ­Ø¯Ø§Øª", "Ø§Ù„ÙˆØ­Ø¯Ù‡": "ÙˆØ­Ø¯Ø§Øª",

    "Ø¨Ø³ÙˆÙ‡Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬", "Ø¨Ø³ÙˆÙ‡Ù€Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬",
    "Ø³ÙˆÙ‡Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬", "Ø³Ù‡Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬",

    "Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡": "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©", "Ù‚Ø§Ù‡Ø±Ù‡": "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©",

    "Ø§Ù„Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡": "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©", "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡": "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©",
    "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©": "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©",

    "Ø§Ù„Ø¬ÙŠØ²Ø©": "Ø¬ÙŠØ²Ù‡", "Ø§Ù„Ø¬ÙŠØ²Ù‡": "Ø¬ÙŠØ²Ù‡", "Ø¬ÙŠØ²Ø©": "Ø¬ÙŠØ²Ù‡",

    # ====================
    #  ÙƒÙ„Ù…Ø§Øª ØªØ¬Ø§Ø±ÙŠØ© Ù…ÙˆØ­Ø¯Ø©
    # ====================
    "Ù„Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª", "Ø§Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª", "Ù…Ù‚Ø§ÙˆÙ„ÙˆÙ†": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª",
    "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª Ø¹Ø§Ù…Ø©": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª", "Ø§Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª",

    "Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª": "ØµÙ†Ø§Ø¹Ø§Øª", "Ø§Ù„ØµÙ†Ø§Ø¹Ø§Øª": "ØµÙ†Ø§Ø¹Ø§Øª",
    "ØµÙ†Ø§Ø¹ÙŠÙ‡": "ØµÙ†Ø§Ø¹ÙŠØ©",

    "Ù„Ù„Ø®Ø¯Ù…Ø§Øª": "Ø®Ø¯Ù…Ø§Øª", "Ø§Ù„Ø®Ø¯Ù…Ø§Øª": "Ø®Ø¯Ù…Ø§Øª",
    "Ø§Ù„Ø®Ø¯Ù…ÙŠØ©": "Ø®Ø¯Ù…Ø§Øª",

    "Ù„Ù„ØªØ¬Ø§Ø±Ø©": "ØªØ¬Ø§Ø±Ø©", "Ø§Ù„ØªØ¬Ø§Ø±Ø©": "ØªØ¬Ø§Ø±Ø©",
    "ØªØ¬Ø§Ø±ÙŠØ©": "ØªØ¬Ø§Ø±Ø©",

    "Ù„Ù„ØªÙ†Ù…ÙŠØ©": "ØªÙ†Ù…ÙŠØ©", "Ø§Ù„ØªÙ†Ù…ÙŠØ©": "ØªÙ†Ù…ÙŠØ©",

    "Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±": "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ©": "Ø§Ø³ØªØ«Ù…Ø§Ø±",

    "Ù„Ù„ØªØ·ÙˆÙŠØ±": "ØªØ·ÙˆÙŠØ±", "Ø§Ù„ØªØ·ÙˆÙŠØ±": "ØªØ·ÙˆÙŠØ±",

    "Ù„Ù„ØªÙˆØ±ÙŠØ¯Ø§Øª": "ØªÙˆØ±ÙŠØ¯Ø§Øª", "Ø§Ù„ØªÙˆØ±ÙŠØ¯Ø§Øª": "ØªÙˆØ±ÙŠØ¯Ø§Øª",
    "ØªÙˆØ±ÙŠØ¯": "ØªÙˆØ±ÙŠØ¯Ø§Øª",

    "Ù„Ù„Ø§Ù†ØªØ§Ø¬": "Ø§Ù†ØªØ§Ø¬", "Ø§Ù„Ø§Ù†ØªØ§Ø¬": "Ø§Ù†ØªØ§Ø¬",

    "Ù„Ù„ØªÙˆØ²ÙŠØ¹": "ØªÙˆØ²ÙŠØ¹", "Ø§Ù„ØªÙˆØ²ÙŠØ¹": "ØªÙˆØ²ÙŠØ¹",

    "Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª": "Ù…Ø¹Ø§Ù„Ø¬Ø©", "Ù…Ø¹Ø§Ù„Ø¬Ù‡": "Ù…Ø¹Ø§Ù„Ø¬Ø©",

    # ====================
    #  ÙƒÙ„Ù…Ø§Øª ØµÙ†Ø§Ø¹ÙŠØ© / ÙÙ†ÙŠØ©
    # ====================
    "Ø§Ù„ÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª": "ÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª", "ÙƒÙŠÙ…Ø§ÙˆÙŠ": "ÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª",
    "ÙƒÙŠÙ…ÙŠØ§ÙˆÙŠØ§Øª": "ÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª",

    "Ø§Ù„Ø¨Ù„Ø§Ø³ØªÙŠÙƒ": "Ø¨Ù„Ø§Ø³ØªÙŠÙƒ", "Ø§Ù„Ø¨Ù„Ø§Ø³ØªÙŠÙƒÙŠØ©": "Ø¨Ù„Ø§Ø³ØªÙŠÙƒ",

    "Ø§Ù„Ø²Ø¬Ø§Ø¬": "Ø²Ø¬Ø§Ø¬", "Ø§Ù„Ø²Ø¬Ø§Ø¬ÙŠÙ‡": "Ø²Ø¬Ø§Ø¬",

    "Ø§Ù„Ø§Ø®Ø´Ø§Ø¨": "Ø§Ø®Ø´Ø§Ø¨", "Ø®Ø´Ø¨": "Ø§Ø®Ø´Ø§Ø¨",

    "Ø§Ù„Ø¨ÙˆÙŠØ§Øª": "Ø¨ÙˆÙŠØ§", "Ø¯Ù‡Ø§Ù†Ø§Øª": "Ø¨ÙˆÙŠØ§",

    "Ø§Ù„ÙˆØ±Ù‚": "ÙˆØ±Ù‚", "Ø§ÙˆØ±Ø§Ù‚": "ÙˆØ±Ù‚", "ÙˆØ±Ù‚ÙŠÙ‡": "ÙˆØ±Ù‚",

    "Ø§Ù„Ø­Ø¯ÙŠØ¯": "Ø­Ø¯ÙŠØ¯", "Ø­Ø¯ÙŠØ¯ÙŠØ©": "Ø­Ø¯ÙŠØ¯",

    "Ø§Ù„Ø§Ø³Ù…Ù†Øª": "Ø§Ø³Ù…Ù†Øª", "Ø§Ø³Ù…Ù†ØªÙŠØ©": "Ø§Ø³Ù…Ù†Øª",

    # ====================
    #  Ø£ØºØ°ÙŠØ© ÙˆÙ…Ø´Ø±ÙˆØ¨Ø§Øª
    # ====================
    "Ø§ØºØ°ÙŠØ©": "Ø§ØºØ°ÙŠÙ‡", "Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©": "Ø§ØºØ°ÙŠÙ‡", "Ø§Ù„ØºØ°Ø§Ø¦ÙŠÙ‡": "Ø§ØºØ°ÙŠÙ‡",
    "Ù„Ù„Ø§ØºØ°ÙŠØ©": "Ø§ØºØ°ÙŠÙ‡",

    "Ù…Ø®Ø¨ÙˆØ²Ø§Øª": "Ù…Ø®Ø¨ÙˆØ²Ø§Øª", "Ø¨Ø³ÙƒÙˆÙŠØª": "Ù…Ø®Ø¨ÙˆØ²Ø§Øª",

    "Ø§Ù„Ø¨Ø§Ù†": "Ø§Ù„Ø¨Ø§Ù†", "Ø§Ù„Ø§Ù„Ø¨Ø§Ù†": "Ø§Ù„Ø¨Ø§Ù†",

    "Ø§Ù„Ø¹Ø¬Ø§Ø¦Ù†": "Ù…ÙƒØ±ÙˆÙ†Ø©", "Ù…Ø¹Ø¬Ù†Ø§Øª": "Ù…ÙƒØ±ÙˆÙ†Ø©",

    "Ø§Ù„Ù„Ø­ÙˆÙ…": "Ù„Ø­ÙˆÙ…", "Ù„Ø­ÙˆÙ…": "Ù„Ø­ÙˆÙ…",

    # ====================
    #  ÙƒÙ‡Ø±Ø¨Ø§Ø¡ / Ø·Ø§Ù‚Ø©
    # ====================
    "ÙƒÙ‡Ø±Ø¨Ø§Ø¡": "ÙƒÙ‡Ø±Ø¨Ø§Ø¡", "ÙƒÙ‡Ø±Ø¨ÙŠØ©": "ÙƒÙ‡Ø±Ø¨Ø§Ø¡",
    "Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª": "Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª",

    "Ù…Ø­ÙˆÙ„Ø§Øª": "Ù…Ø­ÙˆÙ„Ø§Øª", "Ù…Ø­ÙˆÙ„": "Ù…Ø­ÙˆÙ„Ø§Øª",

    # ====================
    #  Ù†Ù‚Ù„ / Ø³ÙŠØ§Ø±Ø§Øª
    # ====================
    "Ù„Ù„Ù†Ù‚Ù„": "Ù†Ù‚Ù„", "Ø§Ù„Ù†Ù‚Ù„": "Ù†Ù‚Ù„", "Ø§Ù„Ù†Ù‚Ù„ÙŠØ§Øª": "Ù†Ù‚Ù„",
    "Ø§Ù„Ø´Ø­Ù†": "Ù†Ù‚Ù„",

    "Ù„Ù„Ø³ÙŠØ§Ø±Ø§Øª": "Ø³ÙŠØ§Ø±Ø§Øª", "Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª": "Ø³ÙŠØ§Ø±Ø§Øª",
    "Ø³ÙŠØ§Ø±Ø©": "Ø³ÙŠØ§Ø±Ø§Øª",

    # ====================
    #  Ø²Ø±Ø§Ø¹Ø© / Ø£Ø±Ø§Ø¶ÙŠ
    # ====================
    "Ø§Ù„Ø²Ø±Ø§Ø¹Ù‰": "Ø²Ø±Ø§Ø¹ÙŠ", "Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ": "Ø²Ø±Ø§Ø¹ÙŠ",
    "Ø²Ø±Ø§Ø¹ÙŠØ©": "Ø²Ø±Ø§Ø¹ÙŠ",

    "Ø£Ø±Ø¶ÙŠ": "Ø§Ø±Ø§Ø¶ÙŠ", "Ø§Ù„Ø§Ø±Ø§Ø¶ÙŠ": "Ø§Ø±Ø§Ø¶ÙŠ",

    # ====================
    #  ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ù…Ø©
    # ====================
    "Ø¬Ø±ÙˆØ¨": "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù…Ø¬Ù…ÙˆØ¹Ø©": "Ù…Ø¬Ù…ÙˆØ¹Ø©",

    "Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "Ù‚Ø§Ø¨Ø¶Ø©",

    "Ø§Ù„Ù…ØµØ±ÙŠØ©": "Ù…ØµØ±ÙŠØ©", "Ø§Ù„Ù…ØµØ±ÙŠÙ‡": "Ù…ØµØ±ÙŠØ©",

    "Ø§Ù„Ø¹Ø±Ø¨": "Ø¹Ø±Ø¨ÙŠ", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø¹Ø±Ø¨ÙŠ",
}

STOPWORDS = {
    "Ø´Ø±ÙƒØ©", "Ø§Ù„Ø´Ø±ÙƒØ©", "Ø´Ø±ÙƒÙ‡", "Ø§Ù„Ø´Ø±ÙƒÙ‡",
    "ÙˆØ§Ù„", "Ø¨Ø§Ù„", "Ù„Ù„", "Ù„",
    "Ù…ØµØ±", "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©",
    "Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©", "Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©", "Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©",
    "Ù…ØµÙ†Ø¹", "Ø§Ù„ØµÙ†Ø§Ø¹Ø§Øª", "ØµÙ†Ø§Ø¹ÙŠØ©",
    "Ù„Ù„ØªØ¬Ø§Ø±Ø©", "ØªØ¬Ø§Ø±ÙŠØ©",
    "Ø¬Ø±ÙˆØ¨", "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª",
    "Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©", "Ø§Ù„Ø§ØºØ°ÙŠØ©", "Ø§ØºØ°ÙŠØ©",
    "ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø§Øª",
}

# ============================================================
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡
# ============================================================
def normalize_letters(text):
    if pd.isna(text): 
        return ""
    s = str(text)
    s = re.sub(r"[Ø£Ø¥Ø¢Ø§]", "Ø§", s)
    s = re.sub(r"[Ø©]", "Ù‡", s)
    s = re.sub(r"[Ù‰ÙŠØ¦]", "ÙŠ", s)
    s = re.sub(r"[Ø¤]", "Ùˆ", s)
    s = re.sub(r"[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù€]", "", s)
    return s

def remove_al_prefix(word):
    for pref in ("ÙˆØ§Ù„", "Ø¨Ø§Ù„", "Ù„Ù„", "Ø§Ù„", "Ù„"):
        if word.startswith(pref) and len(word) > len(pref) + 1:
            return word[len(pref):]
    return word

def normalize_name(s):
    if pd.isna(s): 
        return ""
    s = normalize_letters(s).lower()
    s = re.sub(r"[^Ø¡-ÙŠ\s]", " ", s)
    words = [remove_al_prefix(w) for w in s.split() if w.strip()]
    normalized = [WORD_MAP.get(w, w) for w in words]
    final = " ".join(normalized)
    for k, v in WORD_MAP.items():
        final = re.sub(rf"\b{k}\b", v, final)
    return re.sub(r"\s+", " ", final).strip()

def tokenize(s):
    norm = normalize_name(s)
    return set(w for w in norm.split() if w and w not in STOPWORDS)

# ====== Ù†Ø³Ø® Ù…Ø¨Ø³Ø·Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ (Ø¨Ø¯ÙˆÙ† WORD_MAP) ======
def basic_normalize_name(s):
    if pd.isna(s):
        return ""
    s = normalize_letters(s).lower()
    s = re.sub(r"[^Ø¡-ÙŠ\s]", " ", s)
    words = [remove_al_prefix(w) for w in s.split() if w.strip()]
    words = [w for w in words if w not in STOPWORDS]
    return " ".join(words)

def tokenize_basic(s):
    return [w for w in basic_normalize_name(s).split() if w.strip()]

def fuzzy(a, b):
    return SequenceMatcher(None, a, b).ratio()

def to_num(v):
    try:
        return float(str(v).replace(",", "").strip())
    except:
        return np.nan

def parse_dates(series, dayfirst):
    dt = pd.to_datetime(series, errors="coerce", dayfirst=dayfirst)
    return dt, dt.dt.year.fillna(0).astype(int), dt.dt.month.fillna(0).astype(int)

# ============================================================
# Auto-Learn: Ø¨Ù†Ø§Ø¡ WORD_MAP Ù…Ù‚ØªØ±Ø­ Ù…Ù† Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ø¯Ø§Ø®Ù„ UI)
# ============================================================
def build_auto_word_map(names_series, min_freq=2, sim_threshold=0.9):
    all_tokens = []
    for name in names_series.dropna():
        toks = tokenize_basic(name)
        all_tokens.extend(toks)

    if not all_tokens:
        return [], {}

    freq = {}
    for t in all_tokens:
        freq[t] = freq.get(t, 0) + 1

    vocab = [w for w, c in freq.items() if c >= min_freq]
    vocab_sorted = sorted(vocab, key=lambda w: freq[w], reverse=True)

    suggestions = []
    auto_map = {}

    for i, base in enumerate(vocab_sorted):
        for other in vocab_sorted[i+1:]:
            b, o = base, other
            if freq[o] > freq[b]:
                b, o = o, b

            sim = SequenceMatcher(None, b, o).ratio()
            if sim >= sim_threshold and b != o:
                if o not in auto_map:
                    auto_map[o] = b
                    suggestions.append({
                        "Ø§Ù„ÙƒÙ„Ù…Ø©_Ø§Ù„Ø£Ù‚Ù„_ØªÙƒØ±Ø§Ø±Ù‹Ø§": o,
                        "Ø§Ù„ÙƒÙ„Ù…Ø©_Ø§Ù„Ø£ÙƒØ«Ø±_Ø´ÙŠÙˆØ¹Ù‹Ø§": b,
                        "ØªÙƒØ±Ø§Ø±_Ø§Ù„Ø£Ù‚Ù„": freq[o],
                        "ØªÙƒØ±Ø§Ø±_Ø§Ù„Ø£ÙƒØ«Ø±": freq[b],
                        "Ù†Ø³Ø¨Ø©_Ø§Ù„ØªØ´Ø§Ø¨Ù‡": round(sim, 3),
                    })

    return suggestions, auto_map

# ============================================================
# ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù„ÙØ§Øª
# ============================================================
def prepare_sales(df_raw):
    df = df_raw.copy()
    df["amt"] = df[COL_AMOUNT].apply(to_num)
    
    grouped = df.groupby(COL_INV).agg(
        net_amount=("amt", "sum"),
        pos_date=(
            COL_DATE,
            lambda x: x[df.loc[x.index, "amt"] > 0].iloc[0]
            if any(df.loc[x.index, "amt"] > 0) else np.nan
        ),
        has_return=("amt", lambda s: any(s < 0)),
        name=(COL_NAME, "first"),
    ).reset_index()
    
    grouped = grouped[grouped["net_amount"] > 0]
    grouped["date_parsed"], grouped["year"], grouped["month"] = parse_dates(
        grouped["pos_date"], dayfirst=False
    )
    grouped["name_norm"] = grouped["name"].apply(normalize_name)
    grouped["tokens"] = grouped["name"].apply(tokenize)
    return grouped

def prepare_tax(df_raw):
    df = df_raw.copy()
    df["v_file"] = df[COL_TAX_AMOUNT].apply(to_num)
    df["v_tax_paid"] = df[COL_TAX_TAXED].apply(to_num)
    
    def rate_to_float(x):
        try:
            return float(str(x).replace("%", "").strip()) / 100.0
        except:
            return np.nan
    
    df["rate"] = df[COL_TAX_RATE].apply(rate_to_float)
    df["v_tax"] = df.apply(
        lambda r: r["v_tax_paid"] / r["rate"]
        if pd.notna(r["v_tax_paid"]) and pd.notna(r["rate"]) and r["rate"] > 0
        else np.nan,
        axis=1
    )
    df["v_mix"] = df[["v_file", "v_tax"]].mean(axis=1, skipna=True)
    df["date_parsed"], df["year"], df["month"] = parse_dates(
        df[COL_TAX_DATE], dayfirst=True
    )
    df["name_norm"] = df[COL_TAX_NAME].apply(normalize_name)
    df["tokens"] = df[COL_TAX_NAME].apply(tokenize)
    return df

# ============================================================
# ÙÙ„Ø§ØªØ± Ø§Ù„Ø¨Ø­Ø«
# ============================================================
def filter_year_and_date(sales_df, tax_date, tax_year, tax_month):
    if tax_year == 0 or pd.isna(tax_date):
        return sales_df.iloc[0:0]
    
    allowed_years = [tax_year, tax_year - 1, tax_year - 2]
    mask_year = sales_df["year"].isin(allowed_years)
    mask_date = (sales_df["date_parsed"] <= tax_date)
    
    return sales_df[mask_year & mask_date]

def extended_subset_search(cand, targets, max_invoices=50, max_nodes=200000):
    if not targets: 
        return None
    max_t, min_t = max(targets), min(targets)
    
    cand = cand.head(max_invoices).sort_values("net_amount", ascending=False)
    rows = list(cand.itertuples(index=False))
    n = len(rows)
    if n == 0: 
        return None
    
    amounts = [r.net_amount for r in rows]
    suffix = [0.0] * (n + 1)
    for i in range(n - 1, -1, -1):
        suffix[i] = suffix[i + 1] + amounts[i]
    
    best = None
    best_diff = float("inf")
    nodes = 0
    
    def dfs(i, cur_sum, chosen):
        nonlocal best, best_diff, nodes
        nodes += 1
        if nodes > max_nodes or cur_sum > max_t * 1.05: 
            return
        if cur_sum + suffix[i] < min_t * 0.95: 
            return
        if i == n:
            diff = min(abs(cur_sum - t) for t in targets)
            if diff <= 0.05 * max_t and diff < best_diff:
                best_diff = diff
                best = chosen[:]
            return
        chosen.append(i)
        dfs(i + 1, cur_sum + amounts[i], chosen)
        chosen.pop()
        dfs(i + 1, cur_sum, chosen)
    
    dfs(0, 0.0, [])
    return [rows[i] for i in best] if best else None

# ============================================================
# Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (Ù†ÙØ³Ù‡Ø§ØŒ ØªØ³ØªØ®Ø¯Ù… Ù„Ø§Ø­Ù‚Ù‹Ø§ ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„ØªÙŠÙ†)
# ============================================================
def find_best_match(tax_row, sales_df, used_invoices):
    tax_date = tax_row["date_parsed"]
    if pd.isna(tax_date): 
        return None
    
    v_file, v_tax, v_mix = tax_row["v_file"], tax_row["v_tax"], tax_row["v_mix"]
    targets = [t for t in (v_file, v_tax, v_mix) if pd.notna(t) and t > 0]
    if not targets: 
        return None
    
    cand = filter_year_and_date(sales_df, tax_date, tax_row["year"], tax_row["month"])
    if cand.empty: 
        return None
    
    cand = cand[~cand[COL_INV].astype(str).isin(used_invoices)].copy()
    if cand.empty: 
        return None
    
    cand["token_score"] = cand["tokens"].apply(lambda t: len(t & tax_row["tokens"]))
    cand["fuzzy"] = cand["name_norm"].apply(lambda s: fuzzy(s, tax_row["name_norm"]))
    cand = cand[(cand["token_score"] >= 1) | (cand["fuzzy"] >= 0.75)]
    
    if cand.empty: 
        return None
    
    def within_absolute(val, max_diff=1.0):
        return any(abs(val - t) <= max_diff for t in targets)
    
    def within_pct(val, pct=0.05):
        return any(abs(val - t) <= pct * t for t in targets)
    
    cand["value_dist"] = cand["net_amount"].apply(
        lambda x: min(abs(x - t) for t in targets)
    )
    cand = cand.sort_values(
        by=["value_dist", "token_score", "fuzzy"], ascending=[True, False, False]
    )
    
    # 1. ÙØ§ØªÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹
    for _, r in cand.head(100).iterrows():
        if within_absolute(r["net_amount"], max_diff=1.0):
            return (
                [str(r[COL_INV])],
                [str(r["year"])],
                [str(r["pos_date"])],
                float(r["net_amount"]),
                r["has_return"],
            )
    
    # 2. ÙØ§ØªÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© 5%
    for _, r in cand.head(50).iterrows():
        if within_pct(r["net_amount"]):
            return (
                [str(r[COL_INV])],
                [str(r["year"])],
                [str(r["pos_date"])],
                float(r["net_amount"]),
                r["has_return"],
            )
    
    # 3. Ù…Ø¬Ù…ÙˆØ¹ 2 ÙÙˆØ§ØªÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚
    for combo in combinations(cand.head(80).itertuples(index=False), 2):
        total = sum(r.net_amount for r in combo)
        if not within_absolute(total, max_diff=1.0): 
            continue
        invs = [str(r._asdict()[COL_INV]) for r in combo]
        if len(set(invs)) != len(invs): 
            continue
        years = [str(r.year) for r in combo]
        dates = [str(r.pos_date) for r in combo]
        ret = any(r.has_return for r in combo)
        return invs, years, dates, float(total), ret
    
    # 4. Ù…Ø¬Ù…ÙˆØ¹ 2-3 ÙÙˆØ§ØªÙŠØ± 5%
    for n in [2, 3]:
        for combo in combinations(cand.head(80).itertuples(index=False), n):
            total = sum(r.net_amount for r in combo)
            if not within_pct(total): 
                continue
            invs = [str(r._asdict()[COL_INV]) for r in combo]
            if len(set(invs)) != len(invs): 
                continue
            years = [str(r.year) for r in combo]
            dates = [str(r.pos_date) for r in combo]
            ret = any(r.has_return for r in combo)
            return invs, years, dates, float(total), ret
    
    # 5. Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ Ù„Ù„Ù…Ø¨Ø§Ù„Øº Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
    if max(targets) >= 100000:
        ext = extended_subset_search(cand, targets)
        if ext:
            total = sum(r.net_amount for r in ext)
            if within_pct(total):
                invs = [str(r._asdict()[COL_INV]) for r in ext]
                years = [str(r.year) for r in ext]
                dates = [str(r.pos_date) for r in ext]
                ret = any(r.has_return for r in ext)
                return invs, years, dates, float(total), ret
    
    return None

def match_all_basic(sales_df, tax_df):
    """
    Ù…Ø·Ø§Ø¨Ù‚Ø© ÙƒØ§Ù…Ù„Ø© Ø¹Ø§Ø¯ÙŠØ© (ØªÙØ³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ø¨Ù†Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©)
    """
    used = set()
    result = tax_df.copy()
    for col in NEW_COLS:
        result[col] = ""
    
    matched = 0
    for idx, row in result.iterrows():
        res = find_best_match(row, sales_df, used)
        if res:
            invs, years, dates, amt, has_ret = res
            result.at[idx, NEW_COLS[0]] = " + ".join(invs)
            result.at[idx, NEW_COLS[1]] = " + ".join(years)
            result.at[idx, NEW_COLS[2]] = " + ".join(dates)
            result.at[idx, NEW_COLS[3]] = amt
            result.at[idx, NEW_COLS[4]] = "Ù„Ù‡ Ù…Ø±ØªØ¬Ø¹" if has_ret else ""
            used.update(invs)
            matched += 1
    
    return result, matched, len(result) - matched

# ============================================================
# Ù…Ø±Ø­Ù„Ø© Ø«Ø§Ù†ÙŠØ©: Ù…Ø·Ø§Ø¨Ù‚Ø© Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© + STOPWORDS
# ============================================================
def match_with_user_feedback(
    sales_df_original,
    tax_df_original,
    matches_edited: pd.DataFrame,
    stopwords_edited: pd.DataFrame
):
    """
    - ÙŠØ³ØªØ®Ø¯Ù… Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ© Ø¨Ø¹Ø¯ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    - ÙŠØ³ØªØ®Ø¯Ù… Ù‚Ø§Ø¦Ù…Ø© STOPWORDS Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©
    - ÙŠØ¹ÙŠØ¯ Ø­Ø³Ø§Ø¨ tokens Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… STOPWORDS Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    - ÙŠØ«Ø¨Øª Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„ØªÙŠ ÙˆØ§ÙÙ‚ Ø¹Ù„ÙŠÙ‡Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    - ÙŠÙƒÙ…Ù„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø³Ø·ÙˆØ±
    """

    # 1) ØªØ­Ø¯ÙŠØ« STOPWORDS Ù…Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„
    words = []
    if "ÙƒÙ„Ù…Ø©" in stopwords_edited.columns:
        for v in stopwords_edited["ÙƒÙ„Ù…Ø©"].astype(str).tolist():
            v = v.strip()
            if v:
                words.append(v)
    new_stopwords = set(words)

    global STOPWORDS
    STOPWORDS = new_stopwords

    # 2) Ø¥Ø¹Ø§Ø¯Ø© ØªØ¬Ù‡ÙŠØ² tokens Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ STOPWORDS Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    sales_df = sales_df_original.copy()
    tax_df = tax_df_original.copy()

    sales_df["name_norm"] = sales_df["name"].apply(normalize_name)
    sales_df["tokens"] = sales_df["name"].apply(tokenize)

    tax_df["name_norm"] = tax_df[COL_TAX_NAME].apply(normalize_name)
    tax_df["tokens"] = tax_df[COL_TAX_NAME].apply(tokenize)

    # 3) ØªØ¬Ù‡ÙŠØ² result_df Ùˆ used_invoices
    result = tax_df.copy()
    for col in NEW_COLS:
        result[col] = ""

    used = set()

    # 4) ØªØ«Ø¨ÙŠØª Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„ØªÙŠ ÙˆØ§ÙÙ‚ Ø¹Ù„ÙŠÙ‡Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    if matches_edited is not None and not matches_edited.empty:
        if "row_id" in matches_edited.columns:
            for _, r in matches_edited.iterrows():
                # Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­Ø· Ø¹Ù…ÙˆØ¯ "Ø§Ø¹ØªÙ…Ø§Ø¯_Ø§Ù„ØªØ·Ø§Ø¨Ù‚" ÙˆØ®Ù„Ù‘Ø§Ù‡ False â†’ Ù†Ø³ØªØ¨Ø¹Ø¯ Ø§Ù„ØµÙ
                if "Ø§Ø¹ØªÙ…Ø§Ø¯_Ø§Ù„ØªØ·Ø§Ø¨Ù‚" in matches_edited.columns:
                    if not bool(r.get("Ø§Ø¹ØªÙ…Ø§Ø¯_Ø§Ù„ØªØ·Ø§Ø¨Ù‚", True)):
                        continue

                row_id = int(r["row_id"])
                inv_str = str(r[NEW_COLS[0]]).strip()
                if not inv_str:
                    continue

                invs = [x.strip() for x in inv_str.split("+") if x.strip()]
                years = str(r.get(NEW_COLS[1], "")).split("+")
                dates = str(r.get(NEW_COLS[2], "")).split("+")
                amt = r.get(NEW_COLS[3], np.nan)
                note = r.get(NEW_COLS[4], "")

                result.at[row_id, NEW_COLS[0]] = " + ".join(invs)
                result.at[row_id, NEW_COLS[1]] = " + ".join([y.strip() for y in years])
                result.at[row_id, NEW_COLS[2]] = " + ".join([d.strip() for d in dates])
                result.at[row_id, NEW_COLS[3]] = amt
                result.at[row_id, NEW_COLS[4]] = note

                for inv in invs:
                    used.add(inv)

    # 5) Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ø¨Ø§Ù‚ÙŠ Ø§Ù„ØµÙÙˆÙ
    matched = 0
    for idx, row in result.iterrows():
        if str(result.at[idx, NEW_COLS[0]]).strip():
            matched += 1
            continue

        res = find_best_match(row, sales_df, used)
        if res:
            invs, years, dates, amt, has_ret = res
            result.at[idx, NEW_COLS[0]] = " + ".join(invs)
            result.at[idx, NEW_COLS[1]] = " + ".join(years)
            result.at[idx, NEW_COLS[2]] = " + ".join(dates)
            result.at[idx, NEW_COLS[3]] = amt
            result.at[idx, NEW_COLS[4]] = "Ù„Ù‡ Ù…Ø±ØªØ¬Ø¹" if has_ret else ""
            used.update(invs)
            matched += 1

    not_matched = len(result) - matched
    return result, matched, not_matched

# ============================================================
# ÙˆØ§Ø¬Ù‡Ø© Streamlit
# ============================================================
st.set_page_config(page_title="Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹", layout="wide")

st.title("ğŸ¯ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø°Ù‡Ø¨ÙŠ (Ø®Ø·ÙˆØªÙŠÙ†)")
st.markdown("---")

with st.expander("ğŸ“– ÙÙƒØ±Ø© Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø§Ø®ØªØµØ§Ø±", expanded=False):
    st.markdown("""
1ï¸âƒ£ **Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰**:  
- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙÙŠÙ†  
- ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª  
- Ø¹Ù…Ù„ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø¯Ø¦ÙŠØ© ÙƒØ§Ù…Ù„Ø©  
- Ø¹Ø±Ø¶ Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ© (ØªÙ‚Ø¯Ø± ØªÙ…Ø³Ø­/ØªØ¹Ø¯Ù„ ÙÙŠÙ‡)  
- Ø¹Ø±Ø¶ Ø¬Ø¯ÙˆÙ„ STOPWORDS (ØªÙ‚Ø¯Ø± ØªØ­Ø°Ù/ØªØ¶ÙŠÙ ÙƒÙ„Ù…Ø§Øª)

2ï¸âƒ£ **Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©**:  
- ØªØ¶ØºØ· Ø²Ø± **Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©**  
- Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ¹ÙŠØ¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù† Ø§Ù„Ø£ÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  - STOPWORDS Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©
  - Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„ØªÙŠ ÙˆØ§ÙÙ‚Øª Ø¹Ù„ÙŠÙ‡Ø§ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ (ÙˆÙŠØ«Ø¨ØªÙ‡Ø§)
  - ÙˆÙŠÙƒÙ…Ù„ Ø§Ù„Ø¨Ø§Ù‚ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§
""")

col1, col2 = st.columns(2)
with col1:
    sales_file = st.file_uploader("ğŸ“Š Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª (CSV)", type="csv")
with col2:
    tax_file = st.file_uploader("ğŸ“‘ ÙƒØ´Ù Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ (CSV)", type="csv")

st.markdown("---")

# =======================================
# Ø§Ù„Ø®Ø·ÙˆØ© 1: Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø¯Ø¦ÙŠØ© + Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
# =======================================
if st.button("ğŸš€ Ø§Ù„Ø®Ø·ÙˆØ© 1: Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø¯Ø¦ÙŠØ© ÙˆØ¨Ù†Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©", use_container_width=True):
    if not sales_file or not tax_file:
        st.error("âš ï¸ Ù…Ù† ÙØ¶Ù„Ùƒ Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙÙŠÙ† Ø£ÙˆÙ„Ø§Ù‹!")
        st.stop()
    try:
        with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª..."):
            sales_raw = pd.read_csv(sales_file, encoding="utf-8-sig", dtype=str)
            tax_raw = pd.read_csv(tax_file, encoding="utf-8-sig", dtype=str)

        with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
            sales_prepared = prepare_sales(sales_raw)
            tax_prepared = prepare_tax(tax_raw)

        with st.spinner("ğŸ¯ Ø¬Ø§Ø±ÙŠ Ø¹Ù…Ù„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©..."):
            draft_df, ok, bad = match_all_basic(sales_prepared, tax_prepared)

        # Ø­ÙØ¸ ÙÙŠ session_state Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„Ø®Ø·ÙˆØ© 2
        st.session_state["sales_prepared"] = sales_prepared
        st.session_state["tax_prepared"] = tax_prepared

        # Ù†Ø¶ÙŠÙ row_id Ù„Ø±Ø¨Ø·Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§
        draft_df = draft_df.copy()
        draft_df.insert(0, "row_id", draft_df.index.astype(int))
        st.session_state["draft_df"] = draft_df

        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª ÙÙ‚Ø·
        matches_only = draft_df[draft_df[NEW_COLS[0]] != ""].copy()
        matches_only["Ø§Ø¹ØªÙ…Ø§Ø¯_Ø§Ù„ØªØ·Ø§Ø¨Ù‚"] = True  # Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§ ÙƒÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ù…Ù‚Ø¨ÙˆÙ„Ø©
        st.session_state["matches_table"] = matches_only

        # Ø¬Ø¯ÙˆÙ„ STOPWORDS Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ¹Ø¯ÙŠÙ„
        stopwords_df = pd.DataFrame({"ÙƒÙ„Ù…Ø©": sorted(STOPWORDS)})
        st.session_state["stopwords_table"] = stopwords_df

        st.success(f"âœ… ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©: {ok:,} ØµÙ Ù…Ø·Ø§Ø¨Ù‚ | {bad:,} ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚.")
        st.info("â¬‡ Ø§Ù†Ø²Ù„ Ù„Ù„Ø£Ø³ÙÙ„ Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª ÙˆØ¬Ø¯ÙˆÙ„ STOPWORDS Ø«Ù… Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ 'Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©'.")

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø®Ø·ÙˆØ© 1: {str(e)}")
        st.exception(e)

st.markdown("---")

# =======================================
# Ø¹Ø±Ø¶ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© (Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯Ø©)
# =======================================
if "draft_df" in st.session_state:

    st.subheader("ğŸ§¾ Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ© (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„ØµÙÙˆÙ ØºÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ©)")
    matches_df = st.session_state.get("matches_table", pd.DataFrame())
    if matches_df.empty:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØ·Ø§Ø¨Ù‚Ø§Øª Ù…Ø¨Ø¯Ø¦ÙŠØ© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†.")
    else:
        edited_matches = st.data_editor(
            matches_df,
            key="matches_editor",
            num_rows="dynamic",
            use_container_width=True
        )

    st.subheader("ğŸ§¹ Ø¬Ø¯ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡Ø§ ÙÙŠ Ø§Ù„Ø§Ø³Ù… (STOPWORDS)")
    stopwords_df = st.session_state.get("stopwords_table", pd.DataFrame({"ÙƒÙ„Ù…Ø©": sorted(STOPWORDS)}))
    edited_stopwords = st.data_editor(
        stopwords_df,
        key="stopwords_editor",
        num_rows="dynamic",
        use_container_width=True
    )

    st.markdown("---")

    # ========================
    # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù…Ø·Ø§Ø¨Ù‚Ø© Ù†Ù‡Ø§Ø¦ÙŠØ©
    # ========================
    if st.button("âœ… Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª", use_container_width=True):
        try:
            sales_prepared = st.session_state["sales_prepared"]
            tax_prepared = st.session_state["tax_prepared"]

            # Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø³Ø­ ÙƒÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§ØªØŒ edited_matches Ù…Ù…ÙƒÙ† ØªÙƒÙˆÙ† ÙØ§Ø¶ÙŠØ©
            edited_matches = st.session_state.get("matches_editor", matches_df)
            edited_stopwords = st.session_state.get("matches_editor_stopwords", edited_stopwords) \
                if "matches_editor_stopwords" in st.session_state else edited_stopwords

            with st.spinner("ğŸ” Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª..."):
                final_df, ok2, bad2 = match_with_user_feedback(
                    sales_prepared,
                    tax_prepared,
                    edited_matches if isinstance(edited_matches, pd.DataFrame) else matches_df,
                    edited_stopwords if isinstance(edited_stopwords, pd.DataFrame) else stopwords_df,
                )

            st.success("ğŸ‰ ØªÙ…Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")

            total_rows = len(final_df)
            success_rate = (ok2 / total_rows * 100) if total_rows > 0 else 0.0
            fail_rate = (bad2 / total_rows * 100) if total_rows > 0 else 0.0

            c1, c2, c3 = st.columns(3)
            with c1:
                st.metric("âœ… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§", f"{ok2:,}", delta=f"{success_rate:.1f}%")
            with c2:
                st.metric("âŒ ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§", f"{bad2:,}", delta=f"{fail_rate:.1f}%")
            with c3:
                st.metric("ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©", f"{success_rate:.2f}%")

            st.markdown("---")
            st.markdown("### ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")

            colD1, colD2 = st.columns(2)
            with colD1:
                out_all = io.BytesIO()
                final_df.to_csv(out_all, index=False, encoding="utf-8-sig")
                st.download_button(
                    "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ´Ù Ø§Ù„ÙƒØ§Ù…Ù„ (Ù†Ù‡Ø§Ø¦ÙŠ)",
                    data=out_all.getvalue(),
                    file_name="ÙƒØ´Ù_Ø®ØµÙ…_Ù…Ù†Ø¨Ø¹_Ù…Ø·Ø§Ø¨Ù‚_Ù†Ù‡Ø§Ø¦ÙŠ.csv",
                    mime="text/csv",
                    use_container_width=True
                )

            with colD2:
                unmatched_final = final_df[final_df[NEW_COLS[0]] == ""]
                if not unmatched_final.empty:
                    out_un = io.BytesIO()
                    unmatched_final.to_csv(out_un, index=False, encoding="utf-8-sig")
                    st.download_button(
                        "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ ÙÙ‚Ø· (Ù†Ù‡Ø§Ø¦ÙŠ)",
                        data=out_un.getvalue(),
                        file_name="ØºÙŠØ±_Ù…Ø·Ø§Ø¨Ù‚_Ù†Ù‡Ø§Ø¦ÙŠ_Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                else:
                    st.success("ğŸ‘ ÙƒÙ„ Ø§Ù„ØµÙÙˆÙ ØªÙ… Ù…Ø·Ø§Ø¨Ù‚ØªÙ‡Ø§ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§.")

            st.markdown("### ğŸ‘€ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø£ÙˆÙ„ 10 ØµÙÙˆÙ Ù…Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
            st.dataframe(final_df.head(10), use_container_width=True)

        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {str(e)}")
            st.exception(e)

st.markdown("---")
st.caption("ğŸ’¼ ØªØ·ÙˆÙŠØ±: Ù…Ø­Ø§Ø³Ø¨ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø§ÙŠÙƒÙ„ Ù†Ø¨ÙŠÙ„ | ğŸš€ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© 2025")
