import io
import re
from itertools import combinations
import numpy as np
import pandas as pd
import streamlit as st
from difflib import SequenceMatcher

# ============================================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¹Ø¯ÙŠÙ„ Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©)
# ============================================================
COL_INV = "ÙÙˆØ§ØªÙŠØ±"
COL_DATE = "Ø§Ù„ØªØ§Ø±ÙŠØ®"
COL_NAME = "Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©"
COL_AMOUNT = "ØµØ§ÙÙ‰ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª"

COL_TAX_NAME = "Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©"
COL_TAX_AMOUNT = "Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØµØ§ÙÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„"
COL_TAX_TAXED = "Ù…Ø­ØµÙ„ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¶Ø±ÙŠØ¨Ù‡"
COL_TAX_RATE = "Ù†Ø³Ø¨Ø© Ø§Ù„Ø®ØµÙ…"
COL_TAX_DATE = "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø§Ù…Ù„"

NEW_COLS = [
    "Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø±Ù‚Ù… Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ø³Ù†Ø© Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ù…Ø¨Ù„Øº Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ù„ØªØ­Ù‚Ù‚",
    "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ù† Ø§Ù„Ù…Ø±ØªØ¬Ø¹",
]

# ============================================================
# STOPWORDS & WORD_MAP
# ============================================================
STOPWORDS = {
    "Ø´Ø±ÙƒØ©", "Ø§Ù„Ø´Ø±ÙƒØ©", "Ø´Ø±ÙƒÙ‡", "Ø§Ù„Ø´Ø±ÙƒÙ‡",
    "ÙˆØ§Ù„", "Ø¨Ø§Ù„", "Ù„Ù„", "Ù„", "Ùˆ",
    "Ù…ØµØ±", "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©", "Ù…ØµØ±ÙŠØ©", "Ø§Ù„Ù…ØµØ±ÙŠØ©",
    "Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©", "Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©", "Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©",
    "Ù…ØµÙ†Ø¹", "Ø§Ù„ØµÙ†Ø§Ø¹Ø§Øª", "ØµÙ†Ø§Ø¹ÙŠØ©",
    "Ù„Ù„ØªØ¬Ø§Ø±Ø©", "ØªØ¬Ø§Ø±ÙŠØ©", "ØªØ¬Ø§Ø±Ù‡",
    "Ø¬Ø±ÙˆØ¨", "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª",
    "Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©", "Ø§Ù„Ø§ØºØ°ÙŠØ©", "Ø§ØºØ°ÙŠØ©", "ØºØ°Ø§Ø¦ÙŠØ©",
    "ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø§Øª", "Ù„Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª", "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª",
}

WORD_MAP = {
    "Ø§Ù„Ù…ÙŠØ§Ù‡": "Ù…ÙŠØ§Ù‡", "Ø§Ù„Ù…ÙŠØ§Ø©": "Ù…ÙŠØ§Ù‡", "Ù…ÙŠØ§Ø©": "Ù…ÙŠØ§Ù‡",
    "Ø§Ù„ØµØ±Ù": "ØµØ±Ù", "Ø§Ù„ØµØ±Ù Ø§Ù„ØµØ­ÙŠ": "ØµØ±Ù ØµØ­ÙŠ",
    "Ø§Ù„Ø´Ø±Ø¨": "Ø´Ø±Ø¨",
    "Ø¨Ø³ÙˆÙ‡Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬", "Ø¨Ø³ÙˆÙ‡Ù€Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬",
    "Ø§Ù„Ø²Ø±Ø§Ø¹Ù‰": "Ø²Ø±Ø§Ø¹ÙŠ", "Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ": "Ø²Ø±Ø§Ø¹ÙŠ",
    "Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±": "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ©": "Ø§Ø³ØªØ«Ù…Ø§Ø±",
    "Ù„Ù„ØªÙˆØ±ÙŠØ¯Ø§Øª": "ØªÙˆØ±ÙŠØ¯Ø§Øª", "ØªÙˆØ±ÙŠØ¯": "ØªÙˆØ±ÙŠØ¯Ø§Øª",
    "Ø§Ù„ØºØ°Ø§Ø¦ÙŠÙ‡": "ØºØ°Ø§Ø¦ÙŠØ©", "Ø§ØºØ°ÙŠÙ‡": "ØºØ°Ø§Ø¦ÙŠØ©",
}

def normalize_letters(text):
    if pd.isna(text): return ""
    s = str(text)
    s = re.sub(r"[Ø£Ø¥Ø¢Ø§]", "Ø§", s)
    s = re.sub(r"[Ø©]", "Ù‡", s)
    s = re.sub(r"[Ù‰ÙŠØ¦]", "ÙŠ", s)
    s = re.sub(r"[Ø¤]", "Ùˆ", s)
    s = re.sub(r"[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù€]", "", s)
    return s

def remove_al_prefix(word):
    for pref in ("ÙˆØ§Ù„", "Ø¨Ø§Ù„", "Ù„Ù„", "Ø§Ù„", "Ù„"):
        if word.startswith(pref) and len(word) > len(pref) + 1:
            return word[len(pref):]
    return word

def normalize_name(s):
    if pd.isna(s): return ""
    s = normalize_letters(s).lower()
    s = re.sub(r"[^Ø¡-ÙŠ\s]", " ", s)
    words = [remove_al_prefix(w) for w in s.split() if w.strip()]
    normalized = [WORD_MAP.get(w, w) for w in words]
    final = " ".join(normalized)
    for k, v in WORD_MAP.items():
        final = re.sub(rf"\b{k}\b", v, final)
    return re.sub(r"\s+", " ", final).strip()

def tokenize(s):
    norm = normalize_name(s)
    return set(w for w in norm.split() if w and w not in STOPWORDS)

def fuzzy(a, b):
    return SequenceMatcher(None, a, b).ratio()

def to_num(v):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ…Ø© Ù„Ø±Ù‚Ù… - ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ # Ùˆ NaN"""
    if pd.isna(v): return np.nan
    s = str(v).strip()
    # Ù„Ùˆ Ø§Ù„Ù‚ÙŠÙ…Ø© ###ØŒ Ù†Ø±Ø¬Ø¹ NaN
    if s.startswith("#") or not s: return np.nan
    try:
        return float(s.replace(",", ""))
    except:
        return np.nan

def parse_dates(series, dayfirst):
    dt = pd.to_datetime(series, errors="coerce", dayfirst=dayfirst)
    return dt, dt.dt.year.fillna(0).astype(int), dt.dt.month.fillna(0).astype(int)

# ============================================================
# ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù„ÙØ§Øª - Ù…Ø¹ ØªØ´Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
# ============================================================
def prepare_sales(df_raw):
    st.write(f"ğŸ” Ø£Ø¹Ù…Ø¯Ø© Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª: {list(df_raw.columns)}")
    
    # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    missing = []
    for col in [COL_INV, COL_DATE, COL_NAME, COL_AMOUNT]:
        if col not in df_raw.columns:
            missing.append(col)
    
    if missing:
        st.error(f"âŒ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª: {missing}")
        st.info(f"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©: {list(df_raw.columns)}")
        st.stop()
    
    df = df_raw.copy()
    df["amt"] = df[COL_AMOUNT].apply(to_num)
    
    # Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    valid_amounts = df["amt"].notna().sum()
    st.info(f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ {valid_amounts:,} Ù‚ÙŠÙ…Ø© ØµØ­ÙŠØ­Ø© Ù…Ù† Ø£ØµÙ„ {len(df):,}")
    
    grouped = df.groupby(COL_INV).agg(
        net_amount=("amt", "sum"),
        pos_date=(
            COL_DATE,
            lambda x: x[df.loc[x.index, "amt"] > 0].iloc[0]
            if any(df.loc[x.index, "amt"] > 0) else np.nan
        ),
        has_return=("amt", lambda s: any(s < 0)),
        name=(COL_NAME, "first"),
    ).reset_index()
    
    grouped = grouped[grouped["net_amount"] > 0]
    
    # ğŸ”¥ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙÙŠ Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª Ø¨ØµÙŠØºØ© dd/mm/yyyy
    grouped["date_parsed"], grouped["year"], grouped["month"] = parse_dates(
        grouped["pos_date"], dayfirst=True  # âœ… ØµØ­!
    )
    
    st.info(f"âœ… ØªÙ… ØªØ¬Ù‡ÙŠØ² {len(grouped):,} ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª")
    
    grouped["name_norm"] = grouped["name"].apply(normalize_name)
    grouped["tokens"] = grouped["name"].apply(tokenize)
    return grouped

def prepare_tax(df_raw):
    st.write(f"ğŸ” Ø£Ø¹Ù…Ø¯Ø© ÙƒØ´Ù Ø§Ù„Ø®ØµÙ…: {list(df_raw.columns)}")
    
    # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
    missing = []
    for col in [COL_TAX_NAME, COL_TAX_AMOUNT, COL_TAX_TAXED, COL_TAX_RATE, COL_TAX_DATE]:
        if col not in df_raw.columns:
            missing.append(col)
    
    if missing:
        st.error(f"âŒ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø© Ù…Ù† ÙƒØ´Ù Ø§Ù„Ø®ØµÙ…: {missing}")
        st.info(f"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©: {list(df_raw.columns)}")
        st.stop()
    
    df = df_raw.copy()
    df["v_file"] = df[COL_TAX_AMOUNT].apply(to_num)
    df["v_tax_paid"] = df[COL_TAX_TAXED].apply(to_num)
    
    # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    valid_amounts = df["v_file"].notna().sum()
    st.info(f"âœ… Ù‚ÙŠÙ… ØµØ­ÙŠØ­Ø©: {valid_amounts:,} Ù…Ù† {len(df):,}")
    
    def rate_to_float(x):
        try:
            return float(str(x).replace("%", "").strip()) / 100.0
        except:
            return np.nan
    
    df["rate"] = df[COL_TAX_RATE].apply(rate_to_float)
    df["v_tax"] = df.apply(
        lambda r: r["v_tax_paid"] / r["rate"]
        if pd.notna(r["v_tax_paid"]) and pd.notna(r["rate"]) and r["rate"] > 0
        else np.nan,
        axis=1
    )
    df["v_mix"] = df[["v_file", "v_tax"]].mean(axis=1, skipna=True)
    
    df["date_parsed"], df["year"], df["month"] = parse_dates(
        df[COL_TAX_DATE], dayfirst=True
    )
    
    df["name_norm"] = df[COL_TAX_NAME].apply(normalize_name)
    df["tokens"] = df[COL_TAX_NAME].apply(tokenize)
    return df

# ============================================================
# ÙÙ„Ø§ØªØ± Ø§Ù„Ø¨Ø­Ø«
# ============================================================
def filter_year_and_date(sales_df, tax_date, tax_year, tax_month):
    if tax_year == 0 or pd.isna(tax_date):
        return sales_df.iloc[0:0]
    
    # Ø¨Ø­Ø« ÙÙŠ 3 Ø³Ù†ÙˆØ§Øª
    allowed_years = [tax_year, tax_year - 1, tax_year - 2]
    mask_year = sales_df["year"].isin(allowed_years)
    mask_date = (sales_df["date_parsed"] <= tax_date)
    
    return sales_df[mask_year & mask_date]

def extended_subset_search(cand, targets, max_invoices=50, max_nodes=200000):
    if not targets: return None
    max_t, min_t = max(targets), min(targets)
    
    cand = cand.head(max_invoices).sort_values("net_amount", ascending=False)
    rows = list(cand.itertuples(index=False))
    n = len(rows)
    if n == 0: return None
    
    amounts = [r.net_amount for r in rows]
    suffix = [0.0] * (n + 1)
    for i in range(n - 1, -1, -1):
        suffix[i] = suffix[i + 1] + amounts[i]
    
    best = None
    best_diff = float("inf")
    nodes = 0
    
    def dfs(i, cur_sum, chosen):
        nonlocal best, best_diff, nodes
        nodes += 1
        if nodes > max_nodes or cur_sum > max_t * 1.05: return
        if cur_sum + suffix[i] < min_t * 0.95: return
        if i == n:
            diff = min(abs(cur_sum - t) for t in targets)
            if diff <= 0.05 * max_t and diff < best_diff:
                best_diff = diff
                best = chosen[:]
            return
        chosen.append(i)
        dfs(i + 1, cur_sum + amounts[i], chosen)
        chosen.pop()
        dfs(i + 1, cur_sum, chosen)
    
    dfs(0, 0.0, [])
    return [rows[i] for i in best] if best else None

# ============================================================
# Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ============================================================
def find_best_match(tax_row, sales_df, used_invoices):
    tax_date = tax_row["date_parsed"]
    if pd.isna(tax_date): return None
    
    v_file, v_tax, v_mix = tax_row["v_file"], tax_row["v_tax"], tax_row["v_mix"]
    targets = [t for t in (v_file, v_tax, v_mix) if pd.notna(t) and t > 0]
    if not targets: return None
    
    cand = filter_year_and_date(sales_df, tax_date, tax_row["year"], tax_row["month"])
    if cand.empty: return None
    
    cand = cand[~cand[COL_INV].astype(str).isin(used_invoices)].copy()
    if cand.empty: return None
    
    cand["token_score"] = cand["tokens"].apply(lambda t: len(t & tax_row["tokens"]))
    cand["fuzzy"] = cand["name_norm"].apply(lambda s: fuzzy(s, tax_row["name_norm"]))
    cand = cand[(cand["token_score"] >= 1) | (cand["fuzzy"] >= 0.7)]
    
    if cand.empty: return None
    
    def within_absolute(val, max_diff=5.0):
        return any(abs(val - t) <= max_diff for t in targets)
    
    def within_pct(val, pct=0.05):
        return any(abs(val - t) <= pct * t for t in targets)
    
    cand["value_dist"] = cand["net_amount"].apply(
        lambda x: min(abs(x - t) for t in targets)
    )
    cand = cand.sort_values(
        by=["value_dist", "token_score", "fuzzy"], 
        ascending=[True, False, False]
    )
    
    # 1. ÙØ§ØªÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹ (â‰¤5 Ø¬Ù†ÙŠÙ‡)
    for _, r in cand.head(100).iterrows():
        if within_absolute(r["net_amount"], max_diff=5.0):
            return (
                [str(r[COL_INV])],
                [str(r["year"])],
                [str(r["pos_date"])],
                float(r["net_amount"]),
                r["has_return"],
            )
    
    # 2. ÙØ§ØªÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© 5%
    for _, r in cand.head(50).iterrows():
        if within_pct(r["net_amount"]):
            return (
                [str(r[COL_INV])],
                [str(r["year"])],
                [str(r["pos_date"])],
                float(r["net_amount"]),
                r["has_return"],
            )
    
    # 3. Ù…Ø¬Ù…ÙˆØ¹ 2 ÙÙˆØ§ØªÙŠØ±
    for combo in combinations(cand.head(60).itertuples(index=False), 2):
        total = sum(r.net_amount for r in combo)
        if not (within_absolute(total, 5.0) or within_pct(total)): continue
        invs = [str(r._asdict()[COL_INV]) for r in combo]
        if len(set(invs)) != len(invs): continue
        years = [str(r.year) for r in combo]
        dates = [str(r.pos_date) for r in combo]
        ret = any(r.has_return for r in combo)
        return invs, years, dates, float(total), ret
    
    # 4. Ù…Ø¬Ù…ÙˆØ¹ 3 ÙÙˆØ§ØªÙŠØ±
    for combo in combinations(cand.head(60).itertuples(index=False), 3):
        total = sum(r.net_amount for r in combo)
        if not within_pct(total): continue
        invs = [str(r._asdict()[COL_INV]) for r in combo]
        if len(set(invs)) != len(invs): continue
        years = [str(r.year) for r in combo]
        dates = [str(r.pos_date) for r in combo]
        ret = any(r.has_return for r in combo)
        return invs, years, dates, float(total), ret
    
    # 5. Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹
    if max(targets) >= 50000:
        ext = extended_subset_search(cand, targets)
        if ext:
            total = sum(r.net_amount for r in ext)
            if within_pct(total):
                invs = [str(r._asdict()[COL_INV]) for r in ext]
                years = [str(r.year) for r in ext]
                dates = [str(r.pos_date) for r in ext]
                ret = any(r.has_return for r in ext)
                return invs, years, dates, float(total), ret
    
    return None

def match_all(sales_df, tax_df):
    used = set()
    result = tax_df.copy()
    for col in NEW_COLS:
        result[col] = ""
    
    matched = 0
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total = len(result)
    for idx, row in result.iterrows():
        # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress = (idx + 1) / total
        progress_bar.progress(progress)
        status_text.text(f"â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {idx + 1}/{total}")
        
        res = find_best_match(row, sales_df, used)
        if res:
            invs, years, dates, amt, has_ret = res
            result.at[idx, NEW_COLS[0]] = " + ".join(invs)
            result.at[idx, NEW_COLS[1]] = " + ".join(years)
            result.at[idx, NEW_COLS[2]] = " + ".join(dates)
            result.at[idx, NEW_COLS[3]] = amt
            result.at[idx, NEW_COLS[4]] = "Ù„Ù‡ Ù…Ø±ØªØ¬Ø¹" if has_ret else ""
            used.update(invs)
            matched += 1
    
    progress_bar.empty()
    status_text.empty()
    
    return result, matched, len(result) - matched

# ============================================================
# ÙˆØ§Ø¬Ù‡Ø© Streamlit
# ============================================================
st.set_page_config(page_title="Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹", layout="wide")

st.title("ğŸ¯ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†")
st.markdown("---")

with st.expander("ğŸ“– Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©", expanded=True):
    st.markdown("""
    **âš ï¸ ØªØ£ÙƒØ¯ Ù…Ù†:**
    1. Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: `ÙÙˆØ§ØªÙŠØ±`, `Ø§Ù„ØªØ§Ø±ÙŠØ®`, `Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©`, `ØµØ§ÙÙ‰ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª`
    2. ÙƒØ´Ù Ø§Ù„Ø®ØµÙ… ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰: `Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©`, `Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØµØ§ÙÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„`, `Ù…Ø­ØµÙ„ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¶Ø±ÙŠØ¨Ù‡`, `Ù†Ø³Ø¨Ø© Ø§Ù„Ø®ØµÙ…`, `ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø§Ù…Ù„`
    3. Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© **Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ #** (Ø§ÙØªØ­ Ø§Ù„Ù…Ù„Ù ÙÙŠ Excel ÙˆØ¹Ø±Ù‘Ø¶ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©)
    4. Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø¨ØµÙŠØºØ© ÙˆØ§Ø¶Ø­Ø© (Ù…Ø«Ø§Ù„: 27/09/2018)
    """)

col1, col2 = st.columns(2)
with col1:
    sales_file = st.file_uploader("ğŸ“Š Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª (CSV)", type="csv")
with col2:
    tax_file = st.file_uploader("ğŸ“‘ ÙƒØ´Ù Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ (CSV)", type="csv")

st.markdown("---")

if st.button("ğŸš€ Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©", type="primary", use_container_width=True):
    if not sales_file or not tax_file:
        st.error("âš ï¸ Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙÙŠÙ† Ø£ÙˆÙ„Ø§Ù‹!")
        st.stop()
    
    try:
        st.markdown("### ğŸ“‚ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª")
        sales_raw = pd.read_csv(sales_file, encoding="utf-8-sig", dtype=str)
        tax_raw = pd.read_csv(tax_file, encoding="utf-8-sig", dtype=str)
        
        st.success(f"âœ… ØªÙ… Ù‚Ø±Ø§Ø¡Ø© {len(sales_raw):,} ØµÙ Ù…Ø¨ÙŠØ¹Ø§Øª Ùˆ {len(tax_raw):,} ØµÙ Ø®ØµÙ…")
        
        st.markdown("### ğŸ”„ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        sales_prepared = prepare_sales(sales_raw)
        tax_prepared = prepare_tax(tax_raw)
        
        st.markdown("### ğŸ¯ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©")
        final_df, ok, bad = match_all(sales_prepared, tax_prepared)
        
        st.success("ğŸ‰ ØªÙ…Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        
        success_rate = (ok/(ok+bad)*100) if (ok+bad) > 0 else 0
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("âœ… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚", f"{ok:,}", delta=f"{success_rate:.1f}%")
        with col2:
            st.metric("âŒ ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚", f"{bad:,}")
        with col3:
            st.metric("ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­", f"{success_rate:.2f}%")
        
        st.markdown("---")
        st.markdown("### ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        
        col1, col2 = st.columns(2)
        with col1:
            output = io.BytesIO()
            final_df.to_csv(output, index=False, encoding="utf-8-sig")
            st.download_button(
                "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ´Ù Ø§Ù„ÙƒØ§Ù…Ù„",
                data=output.getvalue(),
                file_name="ÙƒØ´Ù_Ù…Ø·Ø§Ø¨Ù‚.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        with col2:
            unmatched = final_df[final_df[NEW_COLS[0]] == ""]
            if not unmatched.empty:
                out2 = io.BytesIO()
                unmatched.to_csv(out2, index=False, encoding="utf-8-sig")
                st.download_button(
                    "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚",
                    data=out2.getvalue(),
                    file_name="ØºÙŠØ±_Ù…Ø·Ø§Ø¨Ù‚.csv",
                    mime="text/csv",
                    use_container_width=True
                )
        
        st.markdown("### ğŸ‘€ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        st.dataframe(final_df.head(20), use_container_width=True)
        
    except Exception as e:
        st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
        st.exception(e)

st.markdown("---")
st.caption("ğŸ’¼ Ù…Ø­Ø§Ø³Ø¨ Ù‚Ø§Ù†ÙˆÙ†ÙŠ: Ù…Ø§ÙŠÙƒÙ„ Ù†Ø¨ÙŠÙ„ | ğŸš€ 2025")
