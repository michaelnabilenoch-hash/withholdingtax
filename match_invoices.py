import io
import re
from itertools import combinations
import numpy as np
import pandas as pd
import streamlit as st
from difflib import SequenceMatcher

# ============================================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
# ============================================================
COL_INV = "ÙÙˆØ§ØªÙŠØ±"
COL_DATE = "Ø§Ù„ØªØ§Ø±ÙŠØ®"
COL_NAME = "Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©"
COL_AMOUNT = "ØµØ§ÙÙ‰ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª"

COL_TAX_NAME = "Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©"
COL_TAX_AMOUNT = "Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØµØ§ÙÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„"
COL_TAX_TAXED = "Ù…Ø­ØµÙ„ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¶Ø±ÙŠØ¨Ù‡"
COL_TAX_RATE = "Ù†Ø³Ø¨Ø© Ø§Ù„Ø®ØµÙ…"
COL_TAX_DATE = "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø§Ù…Ù„"

NEW_COLS = [
    "Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø±Ù‚Ù… Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ø³Ù†Ø© Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ù…Ø¨Ù„Øº Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ù„ØªØ­Ù‚Ù‚",
    "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ù† Ø§Ù„Ù…Ø±ØªØ¬Ø¹",
]

# ============================================================
# Ø¯ÙˆØ§Ù„ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
# ============================================================
WORD_MAP = {

    # ====================
    #  Ø­Ø±ÙˆÙ ÙˆØªØ¨Ø¯ÙŠÙ„Ø§Øª Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©
    # ====================
    "Ø§Ù„Ù…ÙŠØ§Ù‡": "Ù…ÙŠØ§Ù‡", "Ø§Ù„Ù…ÙŠØ§Ø©": "Ù…ÙŠØ§Ù‡", "Ù…ÙŠØ§Ø©": "Ù…ÙŠØ§Ù‡",
    "Ø§Ù„Ù…Ø§Ø¡": "Ù…ÙŠØ§Ù‡", "Ù…Ø§Ø¦": "Ù…ÙŠØ§Ù‡",

    "Ø§Ù„ØµØ±Ù": "ØµØ±Ù", "Ø§Ù„ØµØ±Ù‚": "ØµØ±Ù",
    "Ø§Ù„ØµØ±Ù Ø§Ù„ØµØ­ÙŠ": "ØµØ±Ù ØµØ­ÙŠ", "Ø§Ù„ØµØ±Ù Ø§Ù„ØµØ­Ù‰": "ØµØ±Ù ØµØ­ÙŠ",
    "ØµØ±Ù Ø§Ù„ØµØ­ÙŠ": "ØµØ±Ù ØµØ­ÙŠ", "ØµØ±Ù Ø§Ù„ØµØ­Ù‰": "ØµØ±Ù ØµØ­ÙŠ",

    "Ø§Ù„Ø´Ø±Ø¨": "Ø´Ø±Ø¨", "Ø§Ù„Ø´Ø±Ø§Ø¨": "Ø´Ø±Ø¨", "Ø´Ø±Ø§Ø¨": "Ø´Ø±Ø¨",

    "Ø§Ù„ÙˆØ­Ø¯Ø§Øª": "ÙˆØ­Ø¯Ø§Øª", "Ø§Ù„ÙˆØ­Ø¯Ù‡": "ÙˆØ­Ø¯Ø§Øª",

    "Ø¨Ø³ÙˆÙ‡Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬", "Ø¨Ø³ÙˆÙ‡Ù€Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬",
    "Ø³ÙˆÙ‡Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬", "Ø³Ù‡Ø§Ø¬": "Ø¨Ø³ÙˆÙ‡Ø§Ø¬",

    "Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡": "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©", "Ù‚Ø§Ù‡Ø±Ù‡": "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©",

    "Ø§Ù„Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡": "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©", "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡": "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©",
    "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©": "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©", "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡": "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©",

    "Ø§Ù„Ø¬ÙŠØ²Ø©": "Ø¬ÙŠØ²Ù‡", "Ø§Ù„Ø¬ÙŠØ²Ù‡": "Ø¬ÙŠØ²Ù‡", "Ø¬ÙŠØ²Ø©": "Ø¬ÙŠØ²Ù‡",

    # ====================
    #  ÙƒÙ„Ù…Ø§Øª ØªØ¬Ø§Ø±ÙŠØ© Ù…ÙˆØ­Ø¯Ø©
    # ====================
    "Ù„Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª", "Ø§Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª", "Ù…Ù‚Ø§ÙˆÙ„ÙˆÙ†": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª",
    "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª Ø¹Ø§Ù…Ø©": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª", "Ø§Ù„Ù…Ù‚Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©": "Ù…Ù‚Ø§ÙˆÙ„Ø§Øª",

    "Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª": "ØµÙ†Ø§Ø¹Ø§Øª", "Ø§Ù„ØµÙ†Ø§Ø¹Ø§Øª": "ØµÙ†Ø§Ø¹Ø§Øª",
    "ØµÙ†Ø§Ø¹ÙŠÙ‡": "ØµÙ†Ø§Ø¹ÙŠØ©",

    "Ù„Ù„Ø®Ø¯Ù…Ø§Øª": "Ø®Ø¯Ù…Ø§Øª", "Ø§Ù„Ø®Ø¯Ù…Ø§Øª": "Ø®Ø¯Ù…Ø§Øª",
    "Ø§Ù„Ø®Ø¯Ù…ÙŠØ©": "Ø®Ø¯Ù…Ø§Øª",

    "Ù„Ù„ØªØ¬Ø§Ø±Ø©": "ØªØ¬Ø§Ø±Ø©", "Ø§Ù„ØªØ¬Ø§Ø±Ø©": "ØªØ¬Ø§Ø±Ø©",
    "ØªØ¬Ø§Ø±ÙŠØ©": "ØªØ¬Ø§Ø±Ø©",

    "Ù„Ù„ØªÙ†Ù…ÙŠØ©": "ØªÙ†Ù…ÙŠØ©", "Ø§Ù„ØªÙ†Ù…ÙŠØ©": "ØªÙ†Ù…ÙŠØ©",

    "Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±": "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ©": "Ø§Ø³ØªØ«Ù…Ø§Ø±",

    "Ù„Ù„ØªØ·ÙˆÙŠØ±": "ØªØ·ÙˆÙŠØ±", "Ø§Ù„ØªØ·ÙˆÙŠØ±": "ØªØ·ÙˆÙŠØ±",

    "Ù„Ù„ØªÙˆØ±ÙŠØ¯Ø§Øª": "ØªÙˆØ±ÙŠØ¯Ø§Øª", "Ø§Ù„ØªÙˆØ±ÙŠØ¯Ø§Øª": "ØªÙˆØ±ÙŠØ¯Ø§Øª",
    "ØªÙˆØ±ÙŠØ¯": "ØªÙˆØ±ÙŠØ¯Ø§Øª",

    "Ù„Ù„Ø§Ù†ØªØ§Ø¬": "Ø§Ù†ØªØ§Ø¬", "Ø§Ù„Ø§Ù†ØªØ§Ø¬": "Ø§Ù†ØªØ§Ø¬",

    "Ù„Ù„ØªÙˆØ²ÙŠØ¹": "ØªÙˆØ²ÙŠØ¹", "Ø§Ù„ØªÙˆØ²ÙŠØ¹": "ØªÙˆØ²ÙŠØ¹",

    "Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª": "Ù…Ø¹Ø§Ù„Ø¬Ø©", "Ù…Ø¹Ø§Ù„Ø¬Ù‡": "Ù…Ø¹Ø§Ù„Ø¬Ø©",

    # ====================
    #  ÙƒÙ„Ù…Ø§Øª ØµÙ†Ø§Ø¹ÙŠØ© / ÙÙ†ÙŠØ©
    # ====================
    "Ø§Ù„ÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª": "ÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª", "ÙƒÙŠÙ…Ø§ÙˆÙŠ": "ÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª",
    "ÙƒÙŠÙ…ÙŠØ§ÙˆÙŠØ§Øª": "ÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª",

    "Ø§Ù„Ø¨Ù„Ø§Ø³ØªÙŠÙƒ": "Ø¨Ù„Ø§Ø³ØªÙŠÙƒ", "Ø§Ù„Ø¨Ù„Ø§Ø³ØªÙŠÙƒÙŠØ©": "Ø¨Ù„Ø§Ø³ØªÙŠÙƒ",

    "Ø§Ù„Ø²Ø¬Ø§Ø¬": "Ø²Ø¬Ø§Ø¬", "Ø§Ù„Ø²Ø¬Ø§Ø¬ÙŠÙ‡": "Ø²Ø¬Ø§Ø¬",

    "Ø§Ù„Ø§Ø®Ø´Ø§Ø¨": "Ø§Ø®Ø´Ø§Ø¨", "Ø®Ø´Ø¨": "Ø§Ø®Ø´Ø§Ø¨",

    "Ø§Ù„Ø¨ÙˆÙŠØ§Øª": "Ø¨ÙˆÙŠØ§", "Ø¯Ù‡Ø§Ù†Ø§Øª": "Ø¨ÙˆÙŠØ§",

    "Ø§Ù„ÙˆØ±Ù‚": "ÙˆØ±Ù‚", "Ø§ÙˆØ±Ø§Ù‚": "ÙˆØ±Ù‚", "ÙˆØ±Ù‚ÙŠÙ‡": "ÙˆØ±Ù‚",

    "Ø§Ù„Ø­Ø¯ÙŠØ¯": "Ø­Ø¯ÙŠØ¯", "Ø­Ø¯ÙŠØ¯ÙŠØ©": "Ø­Ø¯ÙŠØ¯",

    "Ø§Ù„Ø§Ø³Ù…Ù†Øª": "Ø§Ø³Ù…Ù†Øª", "Ø§Ø³Ù…Ù†ØªÙŠØ©": "Ø§Ø³Ù…Ù†Øª",

    # ====================
    #  Ø£ØºØ°ÙŠØ© ÙˆÙ…Ø´Ø±ÙˆØ¨Ø§Øª
    # ====================
    "Ø§ØºØ°ÙŠØ©": "Ø§ØºØ°ÙŠÙ‡", "Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©": "Ø§ØºØ°ÙŠÙ‡", "Ø§Ù„ØºØ°Ø§Ø¦ÙŠÙ‡": "Ø§ØºØ°ÙŠÙ‡",
    "Ù„Ù„Ø§ØºØ°ÙŠØ©": "Ø§ØºØ°ÙŠÙ‡",

    "Ù…Ø®Ø¨ÙˆØ²Ø§Øª": "Ù…Ø®Ø¨ÙˆØ²Ø§Øª", "Ø¨Ø³ÙƒÙˆÙŠØª": "Ù…Ø®Ø¨ÙˆØ²Ø§Øª",

    "Ø§Ù„Ø¨Ø§Ù†": "Ø§Ù„Ø¨Ø§Ù†", "Ø§Ù„Ø§Ù„Ø¨Ø§Ù†": "Ø§Ù„Ø¨Ø§Ù†",

    "Ø§Ù„Ø¹Ø¬Ø§Ø¦Ù†": "Ù…ÙƒØ±ÙˆÙ†Ø©", "Ù…Ø¹Ø¬Ù†Ø§Øª": "Ù…ÙƒØ±ÙˆÙ†Ø©",

    "Ø§Ù„Ù„Ø­ÙˆÙ…": "Ù„Ø­ÙˆÙ…", "Ù„Ø­ÙˆÙ…": "Ù„Ø­ÙˆÙ…",

    # ====================
    #  ÙƒÙ‡Ø±Ø¨Ø§Ø¡ / Ø·Ø§Ù‚Ø©
    # ====================
    "ÙƒÙ‡Ø±Ø¨Ø§Ø¡": "ÙƒÙ‡Ø±Ø¨Ø§Ø¡", "ÙƒÙ‡Ø±Ø¨ÙŠØ©": "ÙƒÙ‡Ø±Ø¨Ø§Ø¡",
    "Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª": "Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª",

    "Ù…Ø­ÙˆÙ„Ø§Øª": "Ù…Ø­ÙˆÙ„Ø§Øª", "Ù…Ø­ÙˆÙ„": "Ù…Ø­ÙˆÙ„Ø§Øª",

    # ====================
    #  Ù†Ù‚Ù„ / Ø³ÙŠØ§Ø±Ø§Øª
    # ====================
    "Ù„Ù„Ù†Ù‚Ù„": "Ù†Ù‚Ù„", "Ø§Ù„Ù†Ù‚Ù„": "Ù†Ù‚Ù„", "Ø§Ù„Ù†Ù‚Ù„ÙŠØ§Øª": "Ù†Ù‚Ù„",
    "Ø§Ù„Ø´Ø­Ù†": "Ù†Ù‚Ù„",

    "Ù„Ù„Ø³ÙŠØ§Ø±Ø§Øª": "Ø³ÙŠØ§Ø±Ø§Øª", "Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª": "Ø³ÙŠØ§Ø±Ø§Øª",
    "Ø³ÙŠØ§Ø±Ø©": "Ø³ÙŠØ§Ø±Ø§Øª",

    # ====================
    #  Ø²Ø±Ø§Ø¹Ø© / Ø£Ø±Ø§Ø¶ÙŠ
    # ====================
    "Ø§Ù„Ø²Ø±Ø§Ø¹Ù‰": "Ø²Ø±Ø§Ø¹ÙŠ", "Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ": "Ø²Ø±Ø§Ø¹ÙŠ",
    "Ø²Ø±Ø§Ø¹ÙŠØ©": "Ø²Ø±Ø§Ø¹ÙŠ",

    "Ø£Ø±Ø¶ÙŠ": "Ø§Ø±Ø§Ø¶ÙŠ", "Ø§Ù„Ø§Ø±Ø§Ø¶ÙŠ": "Ø§Ø±Ø§Ø¶ÙŠ",

    # ====================
    #  ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ù…Ø©
    # ====================
    "Ø¬Ø±ÙˆØ¨": "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù…Ø¬Ù…ÙˆØ¹Ø©": "Ù…Ø¬Ù…ÙˆØ¹Ø©",

    "Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "Ù‚Ø§Ø¨Ø¶Ø©",

    "Ø§Ù„Ù…ØµØ±ÙŠØ©": "Ù…ØµØ±ÙŠØ©", "Ø§Ù„Ù…ØµØ±ÙŠÙ‡": "Ù…ØµØ±ÙŠØ©",

    "Ø§Ù„Ø¹Ø±Ø¨": "Ø¹Ø±Ø¨ÙŠ", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ø¹Ø±Ø¨ÙŠ",
}


def normalize_letters(text):
    if pd.isna(text): return ""
    s = str(text)
    s = re.sub(r"[Ø£Ø¥Ø¢Ø§]", "Ø§", s)
    s = re.sub(r"[Ø©]", "Ù‡", s)
    s = re.sub(r"[Ù‰ÙŠØ¦]", "ÙŠ", s)
    s = re.sub(r"[Ø¤]", "Ùˆ", s)
    s = re.sub(r"[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù€]", "", s)
    return s

def remove_al_prefix(word):
    for pref in ("ÙˆØ§Ù„", "Ø¨Ø§Ù„", "Ù„Ù„", "Ø§Ù„", "Ù„"):
        if word.startswith(pref) and len(word) > len(pref) + 1:
            return word[len(pref):]
    return word

def normalize_name(s):
    if pd.isna(s): return ""
    s = normalize_letters(s).lower()
    s = re.sub(r"[^Ø¡-ÙŠ\s]", " ", s)
    words = [remove_al_prefix(w) for w in s.split() if w.strip()]
    normalized = [WORD_MAP.get(w, w) for w in words]
    final = " ".join(normalized)
    for k, v in WORD_MAP.items():
        final = re.sub(rf"\b{k}\b", v, final)
    return re.sub(r"\s+", " ", final).strip()

def tokenize(s):
    norm = normalize_name(s)
    return set(w for w in norm.split() if w and w not in STOPWORDS)

def fuzzy(a, b):
    return SequenceMatcher(None, a, b).ratio()

def to_num(v):
    try:
        return float(str(v).replace(",", "").strip())
    except:
        return np.nan

def parse_dates(series, dayfirst):
    dt = pd.to_datetime(series, errors="coerce", dayfirst=dayfirst)
    return dt, dt.dt.year.fillna(0).astype(int), dt.dt.month.fillna(0).astype(int)

# ============================================================
# ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù„ÙØ§Øª
# ============================================================
def prepare_sales(df_raw):
    df = df_raw.copy()
    df["amt"] = df[COL_AMOUNT].apply(to_num)
    
    grouped = df.groupby(COL_INV).agg(
        net_amount=("amt", "sum"),
        pos_date=(COL_DATE, lambda x: x[df.loc[x.index, "amt"] > 0].iloc[0] if any(df.loc[x.index, "amt"] > 0) else np.nan),
        has_return=("amt", lambda s: any(s < 0)),
        name=(COL_NAME, "first"),
    ).reset_index()
    
    grouped = grouped[grouped["net_amount"] > 0]
    grouped["date_parsed"], grouped["year"], grouped["month"] = parse_dates(grouped["pos_date"], dayfirst=True)
    grouped["name_norm"] = grouped["name"].apply(normalize_name)
    grouped["tokens"] = grouped["name"].apply(tokenize)
    return grouped

def prepare_tax(df_raw):
    df = df_raw.copy()
    df["v_file"] = df[COL_TAX_AMOUNT].apply(to_num)
    df["v_tax_paid"] = df[COL_TAX_TAXED].apply(to_num)
    
    def rate_to_float(x):
        try:
            return float(str(x).replace("%", "").strip()) / 100.0
        except:
            return np.nan
    
    df["rate"] = df[COL_TAX_RATE].apply(rate_to_float)
    df["v_tax"] = df.apply(lambda r: r["v_tax_paid"] / r["rate"] if pd.notna(r["v_tax_paid"]) and pd.notna(r["rate"]) and r["rate"] > 0 else np.nan, axis=1)
    df["v_mix"] = df[["v_file", "v_tax"]].mean(axis=1, skipna=True)
    df["date_parsed"], df["year"], df["month"] = parse_dates(df[COL_TAX_DATE], dayfirst=True)
    df["name_norm"] = df[COL_TAX_NAME].apply(normalize_name)
    df["tokens"] = df[COL_TAX_NAME].apply(tokenize)
    return df

# ============================================================
# ÙÙ„Ø§ØªØ± Ø§Ù„Ø¨Ø­Ø«
# ============================================================
def filter_year_and_date(sales_df, tax_date, tax_year, tax_month):
    if tax_year == 0 or pd.isna(tax_date):
        return sales_df.iloc[0:0]
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ 3 Ø³Ù†ÙˆØ§Øª Ù„ØªØºØ·ÙŠØ© Ø§Ù„ØªØ£Ø®ÙŠØ±
    allowed_years = [tax_year, tax_year - 1, tax_year - 2]
    mask_year = sales_df["year"].isin(allowed_years)
    mask_date = (sales_df["date_parsed"] <= tax_date)
    
    return sales_df[mask_year & mask_date]

def extended_subset_search(cand, targets, max_invoices=50, max_nodes=200000):
    if not targets: return None
    max_t, min_t = max(targets), min(targets)
    
    cand = cand.head(max_invoices).sort_values("net_amount", ascending=False)
    rows = list(cand.itertuples(index=False))
    n = len(rows)
    if n == 0: return None
    
    amounts = [r.net_amount for r in rows]
    suffix = [0.0] * (n + 1)
    for i in range(n - 1, -1, -1):
        suffix[i] = suffix[i + 1] + amounts[i]
    
    best = None
    best_diff = float("inf")
    nodes = 0
    
    def dfs(i, cur_sum, chosen):
        nonlocal best, best_diff, nodes
        nodes += 1
        if nodes > max_nodes or cur_sum > max_t * 1.05: return
        if cur_sum + suffix[i] < min_t * 0.95: return
        if i == n:
            diff = min(abs(cur_sum - t) for t in targets)
            if diff <= 0.05 * max_t and diff < best_diff:
                best_diff = diff
                best = chosen[:]
            return
        chosen.append(i)
        dfs(i + 1, cur_sum + amounts[i], chosen)
        chosen.pop()
        dfs(i + 1, cur_sum, chosen)
    
    dfs(0, 0.0, [])
    return [rows[i] for i in best] if best else None

# ============================================================
# Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ============================================================
def find_best_match(tax_row, sales_df, used_invoices):
    tax_date = tax_row["date_parsed"]
    if pd.isna(tax_date): 
        return None
    
    v_file, v_tax, v_mix = tax_row["v_file"], tax_row["v_tax"], tax_row["v_mix"]
    targets = [t for t in (v_file, v_tax, v_mix) if pd.notna(t) and t > 0]
    if not targets: 
        return None
    
    cand = filter_year_and_date(sales_df, tax_date, tax_row["year"], tax_row["month"])
    if cand.empty: 
        return None
    
    cand = cand[~cand[COL_INV].astype(str).isin(used_invoices)].copy()
    if cand.empty: 
        return None
    
    cand["token_score"] = cand["tokens"].apply(lambda t: len(t & tax_row["tokens"]))
    cand["fuzzy"] = cand["name_norm"].apply(lambda s: fuzzy(s, tax_row["name_norm"]))
    cand = cand[(cand["token_score"] >= 1) | (cand["fuzzy"] >= 0.75)]
    
    if cand.empty: 
        return None
    
    def within_absolute(val, max_diff=1.0):
        return any(abs(val - t) <= max_diff for t in targets)
    
    def within_pct(val, pct=0.05):
        return any(abs(val - t) <= pct * t for t in targets)
    
    cand["value_dist"] = cand["net_amount"].apply(lambda x: min(abs(x - t) for t in targets))
    cand = cand.sort_values(by=["value_dist", "token_score", "fuzzy"], ascending=[True, False, False])
    
    # 1. ÙØ§ØªÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹
    for _, r in cand.head(100).iterrows():
        if within_absolute(r["net_amount"], max_diff=1.0):
            return [str(r[COL_INV])], [str(r["year"])], [str(r["pos_date"])], float(r["net_amount"]), r["has_return"]
    
    # 2. ÙØ§ØªÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© 5%
    for _, r in cand.head(50).iterrows():
        if within_pct(r["net_amount"]):
            return [str(r[COL_INV])], [str(r["year"])], [str(r["pos_date"])], float(r["net_amount"]), r["has_return"]
    
    # 3. Ù…Ø¬Ù…ÙˆØ¹ 2 ÙÙˆØ§ØªÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚
    for combo in combinations(cand.head(80).itertuples(index=False), 2):
        total = sum(r.net_amount for r in combo)
        if not within_absolute(total, max_diff=1.0): continue
        invs = [str(r._asdict()[COL_INV]) for r in combo]
        if len(set(invs)) != len(invs): continue
        years = [str(r.year) for r in combo]
        dates = [str(r.pos_date) for r in combo]
        ret = any(r.has_return for r in combo)
        return invs, years, dates, float(total), ret
    
    # 4. Ù…Ø¬Ù…ÙˆØ¹ 2-3 ÙÙˆØ§ØªÙŠØ± 5%
    for n in [2, 3]:
        for combo in combinations(cand.head(80).itertuples(index=False), n):
            total = sum(r.net_amount for r in combo)
            if not within_pct(total): continue
            invs = [str(r._asdict()[COL_INV]) for r in combo]
            if len(set(invs)) != len(invs): continue
            years = [str(r.year) for r in combo]
            dates = [str(r.pos_date) for r in combo]
            ret = any(r.has_return for r in combo)
            return invs, years, dates, float(total), ret
    
    # 5. Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹
    if max(targets) >= 100000:
        ext = extended_subset_search(cand, targets)
        if ext:
            total = sum(r.net_amount for r in ext)
            if within_pct(total):
                invs = [str(r._asdict()[COL_INV]) for r in ext]
                years = [str(r.year) for r in ext]
                dates = [str(r.pos_date) for r in ext]
                ret = any(r.has_return for r in ext)
                return invs, years, dates, float(total), ret
    
    return None

def match_all(sales_df, tax_df):
    used = set()
    result = tax_df.copy()
    for col in NEW_COLS:
        result[col] = ""
    
    matched = 0
    for idx, row in result.iterrows():
        res = find_best_match(row, sales_df, used)
        if res:
            invs, years, dates, amt, has_ret = res
            result.at[idx, NEW_COLS[0]] = " + ".join(invs)
            result.at[idx, NEW_COLS[1]] = " + ".join(years)
            result.at[idx, NEW_COLS[2]] = " + ".join(dates)
            result.at[idx, NEW_COLS[3]] = amt
            result.at[idx, NEW_COLS[4]] = "Ù„Ù‡ Ù…Ø±ØªØ¬Ø¹" if has_ret else ""
            used.update(invs)
            matched += 1
    
    return result, matched, len(result) - matched

# ============================================================
# ÙˆØ§Ø¬Ù‡Ø© Streamlit
# ============================================================
st.set_page_config(page_title="Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹", layout="wide")

st.title("ğŸ¯ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø°Ù‡Ø¨ÙŠ")
st.markdown("---")

# Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª
with st.expander("ğŸ“– ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…", expanded=False):
    st.markdown("""
    **Ø§Ù„Ø®Ø·ÙˆØ§Øª:**
    1. Ø§Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª (CSV) - Ù„Ø§Ø²Ù… ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: `ÙÙˆØ§ØªÙŠØ±`, `Ø§Ù„ØªØ§Ø±ÙŠØ®`, `Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©`, `ØµØ§ÙÙ‰ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª`
    2. Ø§Ø±ÙØ¹ ÙƒØ´Ù Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ (CSV) - Ù„Ø§Ø²Ù… ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰: `Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©`, `Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØµØ§ÙÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„`, `Ù…Ø­ØµÙ„ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¶Ø±ÙŠØ¨Ù‡`, `Ù†Ø³Ø¨Ø© Ø§Ù„Ø®ØµÙ…`, `ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø§Ù…Ù„`
    3. Ø§Ø¶ØºØ· "Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"
    4. Ø­Ù…Ù‘Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø©
    
    **Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:**
    - âœ… Ø¨Ø­Ø« ÙÙŠ 3 Ø³Ù†ÙˆØ§Øª (Ø§Ù„Ø­Ø§Ù„ÙŠØ© + Ø§Ù„Ø³Ù†ØªÙŠÙ† Ø§Ù„Ø³Ø§Ø¨Ù‚ØªÙŠÙ†)
    - âœ… Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹ (ÙØ±Ù‚ â‰¤ 1 Ø¬Ù†ÙŠÙ‡)
    - âœ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    - âœ… Ø¯Ø¹Ù… Ø§Ù„Ù…Ø±ØªØ¬Ø¹Ø§Øª ÙˆØ§Ù„Ù…Ø¨Ø§Ù„Øº Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
    """)

st.markdown("---")

col1, col2 = st.columns(2)
with col1:
    sales_file = st.file_uploader("ğŸ“Š Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª (CSV)", type="csv", help="Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª")
with col2:
    tax_file = st.file_uploader("ğŸ“‘ ÙƒØ´Ù Ø®ØµÙ… Ø§Ù„Ù…Ù†Ø¨Ø¹ (CSV)", type="csv", help="ÙƒØ´Ù Ø§Ù„Ø®ØµÙ… Ù…Ù† Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨")

st.markdown("---")

if st.button("ğŸš€ Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©", type="primary", use_container_width=True):
    if not sales_file or not tax_file:
        st.error("âš ï¸ Ù…Ù† ÙØ¶Ù„Ùƒ Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙÙŠÙ† Ø£ÙˆÙ„Ø§Ù‹!")
        st.stop()
    
    try:
        with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª..."):
            sales_raw = pd.read_csv(sales_file, encoding="utf-8-sig", dtype=str)
            tax_raw = pd.read_csv(tax_file, encoding="utf-8-sig", dtype=str)
            
            st.info(f"ğŸ“Š ØªÙ… Ù‚Ø±Ø§Ø¡Ø© {len(sales_raw):,} ØµÙ Ù…Ù† Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª Ùˆ {len(tax_raw):,} ØµÙ Ù…Ù† ÙƒØ´Ù Ø§Ù„Ø®ØµÙ…")
        
        with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
            sales_prepared = prepare_sales(sales_raw)
            tax_prepared = prepare_tax(tax_raw)
            
            st.info(f"âœ… ØªÙ… ØªØ¬Ù‡ÙŠØ² {len(sales_prepared):,} ÙØ§ØªÙˆØ±Ø© Ù…Ø¨ÙŠØ¹Ø§Øª Ùˆ {len(tax_prepared):,} Ø³Ø·Ø± Ù…Ù† ÙƒØ´Ù Ø§Ù„Ø®ØµÙ…")
        
        with st.spinner("ğŸ¯ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©... (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¯Ù‚Ø§Ø¦Ù‚)"):
            final_df, ok, bad = match_all(sales_prepared, tax_prepared)
        
        st.success("âœ… ØªÙ…Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        st.markdown("### ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                label="âœ… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚",
                value=f"{ok:,}",
                delta=f"{(ok/(ok+bad)*100):.1f}%" if (ok+bad) > 0 else "0%"
            )
        with col2:
            st.metric(
                label="âŒ ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚",
                value=f"{bad:,}",
                delta=f"{(bad/(ok+bad)*100):.1f}%" if (ok+bad) > 0 else "0%"
            )
        with col3:
            success_rate = (ok/(ok+bad)*100) if (ok+bad) > 0 else 0
            st.metric(
                label="ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­",
                value=f"{success_rate:.2f}%"
            )
        
        st.markdown("---")
        
        # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
        st.markdown("### ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            output = io.BytesIO()
            final_df.to_csv(output, index=False, encoding="utf-8-sig")
            st.download_button(
                label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ´Ù Ø§Ù„ÙƒØ§Ù…Ù„",
                data=output.getvalue(),
                file_name="ÙƒØ´Ù_Ø®ØµÙ…_Ù…Ù†Ø¨Ø¹_Ù…Ø·Ø§Ø¨Ù‚.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        with col2:
            unmatched = final_df[final_df[NEW_COLS[0]] == ""]
            if not unmatched.empty:
                out2 = io.BytesIO()
                unmatched.to_csv(out2, index=False, encoding="utf-8-sig")
                st.download_button(
                    label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ ÙÙ‚Ø·",
                    data=out2.getvalue(),
                    file_name="ØºÙŠØ±_Ù…Ø·Ø§Ø¨Ù‚_Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            else:
                st.success("ğŸ‰ ØªÙ…Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø³Ø·ÙˆØ±!")
        
        # Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        st.markdown("---")
        st.markdown("### ğŸ‘€ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ø£ÙˆÙ„ 10 ØµÙÙˆÙ)")
        st.dataframe(final_df.head(10), use_container_width=True)
        
    except Exception as e:
        st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
        st.exception(e)

st.markdown("---")
st.caption("ğŸ’¼ ØªØ·ÙˆÙŠØ±: Ù…Ø­Ø§Ø³Ø¨ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø§ÙŠÙƒÙ„ Ù†Ø¨ÙŠÙ„ | ğŸš€ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© 2025")
